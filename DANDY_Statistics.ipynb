{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1kGesmTsDGRIsI49ZeP7JiT30pWYpztOg","authorship_tag":"ABX9TyOLLWLG9K6obw6YDkvThM9E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["##### OUTLINE OF THIS FILE #####\n","### This file presents the final statistics\n","### meaning those that deal with the ALL results\n","\n","#### COMBINE - Combine the NILM and ANOMALY files ####\n","\n","#### BASELINE RESULTS - Anomaly_Version/Accuracy/Precision/Recall/F1_Score/Normal_%/Anomaly_% ####\n","#### NILM STATISTICS ####\n","\n","#### TIMING AND MEMORY ####\n","#### TIMING AND MEMORY - AGGREGATE ####\n","\n","#### BOXPLOT - Distribution of Percentage Anomaly per Anomaly Type ####\n","#### BOXPLOT - Distribution of Percentage Anomaly per Anomaly Type - \"Median\" ####"],"metadata":{"id":"PxyAnp3o9ZJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGRP5SUDp7Gx"},"outputs":[],"source":["#### COMBINE - Combine the NILM and ANOMALY files ####\n","###\n","### Combines ANOMALY with NILM based on \"timestmap\"\n","###\n","### Output merged file has:\n","###      - timestamp, active_power, ground_truth_appliance, prediction_appliance, ground_truth_anomaly, prediction_anomaly\n","###\n","### FOLDERS ARE:\n","###    ANOMALY_DIR = os.path.join(BASE, \"ANOMALY_{AnomalyType}\")\n","###    NILM_DIR    = os.path.join(BASE, \"NILM\")\n","###    OUT_DIR     = os.path.join(BASE, \"COMBINED_{AnomalyType}\")\n","###\n","import os\n","import sys\n","import pandas as pd\n","\n","# ---- Parameters you can tweak if needed ----\n","RESIDENCES = [\n","                \"REFIT_House01\",\n","                \"REFIT_House02\",\n","                \"REFIT_House03\",\n","                \"REFIT_House05\",\n","                \"REFIT_House07\",\n","                \"REFIT_House09\",\n","                \"REFIT_House15\",\n","                \"UKDALE_House01\",\n","                \"UKDALE_House02\",\n","                \"UKDALE_House05\",\n","                \"AMPds2_House01\",\n","                \"GREEND_House00\",\n","                \"GREEND_House01\",\n","                \"GREEND_House03\"\n","            ]\n","APPLIANCES = [\"Fridge\", \"WashingMachine\", \"Dishwasher\"]\n","ANOMALIES  = [\"StepChange\", \"MultiStepChange\", \"Mirror\", \"Repeating\", \"StuckMAX\", \"StuckMIN\", \"PowerCycling\"]\n","MODELS     = [\"DIFFUSION_RESIDUALSPECTRAL\"] #\"HOLTWINTERS\", \"3SIGMATHRESHOLD\", \"DBSCAN\", \"ISOLATIONFOREST\", \"OCSVM\", \"LSTMAE\", \"COCA\", \"VAE\", \"TRANAD\"]\n","\n","BASE = \"/content/drive/MyDrive/Paper02_14Datasets\"\n","ANOMALY_DIR = os.path.join(BASE, \"ANOMALY_DIFFUSION_RESIDUALSPECTRAL\")\n","NILM_DIR    = os.path.join(BASE, \"NILM_version_20260207\")\n","OUT_DIR     = os.path.join(BASE, \"COMBINED_DIFFUSION_RESIDUALSPECTRAL\")\n","\n","# Ensure output directory exists\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","def _read_csv_with_dt(path):\n","    \"\"\"Read a CSV and parse 'timestamp' if present. Returns DataFrame or None if missing.\"\"\"\n","    if not os.path.exists(path):\n","        print(f\"[MISS] {path}\")\n","        return None\n","    try:\n","        df = pd.read_csv(path)\n","    except Exception as e:\n","        print(f\"[ERROR] reading {path}: {e}\")\n","        return None\n","    # Parse timestamp robustly if present\n","    if \"timestamp\" in df.columns:\n","        try:\n","            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=False)\n","        except Exception as e:\n","            print(f\"[WARN] couldn't parse timestamp for {path}: {e}\")\n","    else:\n","        print(f\"[WARN] 'timestamp' column missing in {path}\")\n","    return df\n","\n","def _standardize_columns(df, origin):\n","    \"\"\"\n","    Try to standardize column names so downstream merge succeeds.\n","    - Accepts common variants and renames to the expected names when possible.\n","    \"\"\"\n","    if df is None:\n","        return None\n","\n","    # Build rename map cautiously (only if source columns exist)\n","    rename_map = {}\n","    # Prediction anomaly\n","    for cand in [\"prediction_anomaly\", \"pred_anomaly\", \"predicted_anomaly\", \"anomaly_pred\", \"y_pred_anomaly\"]:\n","        if cand in df.columns:\n","            rename_map[cand] = \"prediction_anomaly\"\n","            break\n","    # Ground truth anomaly\n","    for cand in [\"ground_truth_anomaly\", \"gt_anomaly\", \"true_anomaly\", \"label_anomaly\"]:\n","        if cand in df.columns:\n","            rename_map[cand] = \"ground_truth_anomaly\"\n","            break\n","    # Prediction appliance\n","    for cand in [\"prediction_appliance\", \"pred_appliance\", \"predicted_appliance\", \"appliance_pred\", \"y_pred_appliance\"]:\n","        if cand in df.columns:\n","            rename_map[cand] = \"prediction_appliance\"\n","            break\n","    # Ground truth appliance\n","    for cand in [\"ground_truth_appliance\", \"gt_appliance\", \"true_appliance\", \"label_appliance\"]:\n","        if cand in df.columns:\n","            rename_map[cand] = \"ground_truth_appliance\"\n","            break\n","    # Active power\n","    for cand in [\"active_power\", \"power_active\", \"P_active\", \"real_power\"]:\n","        if cand in df.columns:\n","            rename_map[cand] = \"active_power\"\n","            break\n","\n","    if rename_map:\n","        df = df.rename(columns=rename_map)\n","\n","    # Only keep columns we'll need later (plus timestamp)\n","    keep_cols = [c for c in [\"timestamp\", \"active_power\",\n","                             \"ground_truth_appliance\", \"prediction_appliance\",\n","                             \"ground_truth_anomaly\", \"prediction_anomaly\"]\n","                 if c in df.columns]\n","    # If keep_cols loses all but timestamp, keep timestamp to allow merge\n","    if \"timestamp\" in df.columns and len(keep_cols) == 1:\n","        keep_cols = [\"timestamp\"]\n","    df = df[keep_cols] if keep_cols else df\n","    return df\n","\n","def merge_one(residence, appliance, anomaly, model):\n","    # Build file paths\n","    anomaly_path = os.path.join(\n","        ANOMALY_DIR,\n","        f\"{residence}_{appliance}_15minutes_{anomaly}_MERGED_{model}.csv\",\n","    )\n","    nilm_path = os.path.join(\n","        NILM_DIR,\n","        f\"{residence}_{appliance}_15minutes_{anomaly}_NILM.csv\",\n","    )\n","    out_path = os.path.join(\n","        OUT_DIR,\n","        f\"{residence}_{appliance}_15minutes_{anomaly}_COMBINED_{model}.csv\",\n","    )\n","\n","    # Read\n","    df_anom = _read_csv_with_dt(anomaly_path)\n","    df_nilm = _read_csv_with_dt(nilm_path)\n","\n","    if df_anom is None or df_nilm is None:\n","        print(f\"[SKIP] Missing input for {residence}/{appliance}/{anomaly}/{model}\")\n","        return\n","\n","    # Standardize columns / prune extras\n","    df_anom = _standardize_columns(df_anom, \"ANOMALY\")\n","    df_nilm = _standardize_columns(df_nilm, \"NILM\")\n","\n","    # Inner merge on timestamp (only rows present in both files)\n","    # If timestamp is all NaT in either, the merge will be empty.\n","    try:\n","        merged = pd.merge(df_nilm, df_anom, on=\"timestamp\", how=\"inner\", suffixes=(\"_nilm\", \"_anom\"))\n","    except KeyError:\n","        print(f\"[SKIP] 'timestamp' not in both inputs for {residence}/{appliance}/{anomaly}/{model}\")\n","        return\n","\n","    # Build the final ordered columns, pulling from whichever side has them\n","    # active_power: favor NILM side if present there\n","    if \"active_power_nilm\" in merged.columns:\n","        merged[\"active_power\"] = merged[\"active_power_nilm\"]\n","    elif \"active_power_anom\" in merged.columns:\n","        merged[\"active_power\"] = merged[\"active_power_anom\"]\n","\n","    # Consolidate fields if both sides had them (prefer NILM for appliance truth/pred,\n","    # ANOMALY file for anomaly truth/pred)\n","    def coalesce(cols):\n","        for c in cols:\n","            if c in merged.columns:\n","                return merged[c]\n","        return None\n","\n","    merged[\"ground_truth_appliance\"] = coalesce([\"ground_truth_appliance_nilm\", \"ground_truth_appliance_anom\", \"ground_truth_appliance\"])\n","    merged[\"prediction_appliance\"]   = coalesce([\"prediction_appliance_nilm\", \"prediction_appliance_anom\", \"prediction_appliance\"])\n","    merged[\"ground_truth_anomaly\"]   = coalesce([\"ground_truth_anomaly_anom\", \"ground_truth_anomaly_nilm\", \"ground_truth_anomaly\"])\n","    merged[\"prediction_anomaly\"]     = coalesce([\"prediction_anomaly_anom\", \"prediction_anomaly_nilm\", \"prediction_anomaly\"])\n","\n","    # Keep only requested columns in target order\n","    final_cols = [\"timestamp\", \"active_power\", \"ground_truth_appliance\",\n","                  \"prediction_appliance\", \"ground_truth_anomaly\", \"prediction_anomaly\"]\n","    # Ensure all exist; if any missing, create empty (NaN) column so file shape is consistent\n","    for col in final_cols:\n","        if col not in merged.columns:\n","            merged[col] = pd.NA\n","\n","    merged_final = merged[final_cols].copy()\n","\n","    # Sort by timestamp if possible\n","    if pd.api.types.is_datetime64_any_dtype(merged_final[\"timestamp\"]):\n","        merged_final = merged_final.sort_values(\"timestamp\")\n","        # Write as ISO-like string (keeps seconds if present)\n","        merged_final[\"timestamp\"] = merged_final[\"timestamp\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n","    else:\n","        # Leave as-is if non-datetime strings\n","        pass\n","\n","    # Save (overwrite)\n","    try:\n","        merged_final.to_csv(out_path, index=False)\n","        print(f\"[OK] {out_path}  rows={len(merged_final)}\")\n","    except Exception as e:\n","        print(f\"[ERROR] writing {out_path}: {e}\")\n","\n","def main():\n","    # Optional: allow narrowing via CLI args (residence appliance anomaly model)\n","    # If provided, only process that combination.\n","    if len(sys.argv) == 5:\n","        res, app, anom, mod = sys.argv[1:5]\n","        merge_one(res, app, anom, mod)\n","        return\n","\n","    # Otherwise process the cartesian product\n","    for res in RESIDENCES:\n","        for app in APPLIANCES:\n","            for anom in ANOMALIES:\n","                for mod in MODELS:\n","                    merge_one(res, app, anom, mod)\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"3XSS49DikQdp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### BASELINE RESULTS - Anomaly_Version/Accuracy/Precision/Recall/F1_Score/Normal_%/Anomaly_% ####\n","\n","import os\n","import pandas as pd\n","import glob\n","\n","# ==========================\n","# Configuration\n","# ==========================\n","BASE_DIR = \"/content/drive/MyDrive/Paper02_14Datasets\"\n","OUT_FILE = os.path.join(BASE_DIR, \"STATISTICS\", \"Baseline_Stats.csv\")\n","\n","ANOMALY_VERSIONS = [\n","    \"3SIGMA\",\n","    \"AE\",\n","    \"COCA\",\n","    \"DBSCAN\",\n","    \"ISOLATIONFOREST\",\n","    \"DIFFUSION_RESIDUALSPECTRAL\"\n","    # \"DIFFUSION_NORESIDUALNOSPECTRAL\",\n","]\n","\n","# ==========================\n","# Processing\n","# ==========================\n","all_results = []\n","\n","for version in ANOMALY_VERSIONS:\n","    folder = os.path.join(BASE_DIR, f\"ANOMALY_{version}\", \"Percentiles_Summary\")\n","    files = glob.glob(os.path.join(folder, \"*.csv\"))\n","\n","    if not files:\n","        print(f\"⚠️ No files found for {version}\")\n","        continue\n","\n","    dfs = []\n","\n","    for f in files:\n","        try:\n","            df = pd.read_csv(f)\n","\n","            # --------------------------\n","            # Normalize column names\n","            # --------------------------\n","            if \"F1-Score\" in df.columns:\n","                df.rename(columns={\"F1-Score\": \"F1_Score\"}, inplace=True)\n","            if \"F1 Score\" in df.columns:\n","                df.rename(columns={\"F1 Score\": \"F1_Score\"}, inplace=True)\n","\n","            # Handle DIFFUSION_RESIDUALSPECTRAL naming\n","            if \"Normal_pct\" in df.columns:\n","                df.rename(columns={\"Normal_pct\": \"Normal_%\"}, inplace=True)\n","            if \"Anomaly_pct\" in df.columns:\n","                df.rename(columns={\"Anomaly_pct\": \"Anomaly_%\"}, inplace=True)\n","\n","            dfs.append(\n","                df[\n","                    [\n","                        \"Accuracy\",\n","                        \"Precision\",\n","                        \"Recall\",\n","                        \"F1_Score\",\n","                        \"Normal_%\",\n","                        \"Anomaly_%\",\n","                    ]\n","                ]\n","            )\n","\n","        except Exception as e:\n","            print(f\"⚠️ Skipping file {f} due to error: {e}\")\n","\n","    if not dfs:\n","        continue\n","\n","    merged = pd.concat(dfs, ignore_index=True)\n","    mean_values = merged.mean()\n","    mean_values[\"Anomaly_Version\"] = version\n","    all_results.append(mean_values)\n","\n","# ==========================\n","# Save output\n","# ==========================\n","if all_results:\n","    result_df = pd.DataFrame(all_results)[\n","        [\n","            \"Anomaly_Version\",\n","            \"Accuracy\",\n","            \"Precision\",\n","            \"Recall\",\n","            \"F1_Score\",\n","            \"Normal_%\",\n","            \"Anomaly_%\",\n","        ]\n","    ]\n","\n","    os.makedirs(os.path.dirname(OUT_FILE), exist_ok=True)\n","    result_df.to_csv(OUT_FILE, index=False)\n","    print(f\"✅ Saved summary to {OUT_FILE}\")\n","else:\n","    print(\"⚠️ No valid results found.\")\n","\n"],"metadata":{"id":"Xv5Eyx1GkQbI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lONRKbsdlB1g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### NILM STATISTICS ####\n","import os\n","import pandas as pd\n","\n","# ================================================================\n","# Configuration\n","# ================================================================\n","BASE_DIR = \"/content/drive/MyDrive/Paper02_14Datasets/NILM\"\n","OUT_FILE = \"/content/drive/MyDrive/Paper02_14Datasets/STATISTICS/NILM_Stats.csv\"\n","\n","RESIDENCES = [\n","    \"AMPds2_House01\",\n","    \"GREEND_House00\", \"GREEND_House01\", \"GREEND_House03\",\n","    \"UKDALE_House01\", \"UKDALE_House02\", \"UKDALE_House05\",\n","    \"REFIT_House01\", \"REFIT_House02\", \"REFIT_House03\",\n","    \"REFIT_House05\", \"REFIT_House07\", \"REFIT_House09\", \"REFIT_House15\"\n","]\n","\n","# ================================================================\n","# Compute averages\n","# ================================================================\n","records = []\n","for res in RESIDENCES:\n","    file_path = os.path.join(BASE_DIR, f\"{res}_NILM_Results.csv\")\n","    if not os.path.exists(file_path):\n","        print(f\"⚠️ Missing file: {file_path}\")\n","        continue\n","\n","    df = pd.read_csv(file_path)\n","\n","    # Normalize column names\n","    df.columns = [c.strip().lower() for c in df.columns]\n","\n","    # Extract relevant metrics\n","    cols = [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auc\", \"poc\"]\n","    available = [c for c in cols if c in df.columns]\n","\n","    # Compute averages\n","    averages = {c: df[c].mean() for c in available}\n","    averages[\"residence\"] = res\n","    records.append(averages)\n","\n","# ================================================================\n","# Save results\n","# ================================================================\n","if records:\n","    result_df = pd.DataFrame(records)\n","    result_df = result_df[\n","        [\"residence\", \"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auc\", \"poc\"]\n","    ].round(4)\n","    os.makedirs(os.path.dirname(OUT_FILE), exist_ok=True)\n","    result_df.to_csv(OUT_FILE, index=False)\n","    print(f\"✅ Saved: {OUT_FILE}\")\n","else:\n","    print(\"❌ No valid results found.\")\n"],"metadata":{"id":"-KxPTS21lCbX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MBtkNXtPmDBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" #### TIMING AND MEMORY - TrainingTimeSec/InferenceTimeSec/TrainPeakMB/InferencePeakMB ####\n","import os\n","import pandas as pd\n","\n","# --- Configuration ---\n","BASE = \"/content/drive/MyDrive/Paper02_14Datasets\"\n","OUT_FILE = f\"{BASE}/STATISTICS/Timing_and_Memory.csv\"\n","os.makedirs(os.path.dirname(OUT_FILE), exist_ok=True)\n","\n","anomaly_versions = [\n","    \"3SIGMA\", \"AE\", \"COCA\", \"DBSCAN\", \"ISOLATIONFOREST\",\n","    \"DIFFUSION_RESIDUALSPECTRAL\"\n","]\n","\n","residences = [\n","    \"AMPds2_House01\",\n","    \"GREEND_House00\", \"GREEND_House01\", \"GREEND_House03\",\n","    \"UKDALE_House01\", \"UKDALE_House02\", \"UKDALE_House05\",\n","    \"REFIT_House01\", \"REFIT_House02\", \"REFIT_House03\", \"REFIT_House05\",\n","    \"REFIT_House07\", \"REFIT_House09\", \"REFIT_House15\",\n","]\n","\n","records = []\n","\n","# --- Collect timing and memory info ---\n","for res in residences:\n","    for anom in anomaly_versions:\n","        if anom == \"DIFFUSION_RESIDUALSPECTRAL\":\n","          anom = \"DIFFUSION\"\n","        file_path = f\"{BASE}/ANOMALY_{anom}/Percentiles_Summary/{res}_ANOMALY_{anom}_OUTLINE.csv\"\n","        if not os.path.exists(file_path):\n","            print(f\"⚠️ Missing file: {file_path}\")\n","            continue\n","\n","        try:\n","            df = pd.read_csv(file_path)\n","            # Compute mean across appliances if multiple rows exist\n","            timing_memory = df[[\"TrainingTimeSec\", \"InferenceTimeSec\", \"TrainPeakMB\", \"InferencePeakMB\"]].mean()\n","            records.append({\n","                \"residence\": res,\n","                \"anomaly_version\": anom,\n","                \"TrainingTimeSec\": timing_memory[\"TrainingTimeSec\"],\n","                \"InferenceTimeSec\": timing_memory[\"InferenceTimeSec\"],\n","                \"TrainPeakMB\": timing_memory[\"TrainPeakMB\"],\n","                \"InferencePeakMB\": timing_memory[\"InferencePeakMB\"]\n","            })\n","        except Exception as e:\n","            print(f\"❌ Error reading {file_path}: {e}\")\n","\n","# --- Save to CSV ---\n","out_df = pd.DataFrame(records)\n","out_df.to_csv(OUT_FILE, index=False)\n","print(f\"✅ Saved summary to: {OUT_FILE}\")\n","print(out_df.head())"],"metadata":{"id":"0Y5EaI3LmC8Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Tn0m3RKWV32O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### TIMING AND MEMORY - AGGREGATE ####\n","import pandas as pd\n","\n","# ==============================\n","# Configuration\n","# ==============================\n","input_file = \"/content/drive/MyDrive/Paper02_14Datasets/STATISTICS/Timing_and_Memory.csv\"\n","output_file = \"/content/drive/MyDrive/Paper02_14Datasets/STATISTICS/Timing_and_Memory_aggregate.csv\"\n","\n","# ==============================\n","# Load Data\n","# ==============================\n","df = pd.read_csv(input_file)\n","\n","# ==============================\n","# Compute Averages by Anomaly Version\n","# ==============================\n","agg_df = df.groupby(\"anomaly_version\", as_index=False)[\n","    [\"TrainingTimeSec\", \"InferenceTimeSec\", \"TrainPeakMB\", \"InferencePeakMB\"]\n","].mean()\n","\n","# ==============================\n","# Save to CSV\n","# ==============================\n","agg_df.to_csv(output_file, index=False)\n","\n","print(f\"✅ Aggregated file saved to: {output_file}\")\n","print(agg_df)\n"],"metadata":{"id":"IZ-XqD1TmC55"},"execution_count":null,"outputs":[]}]}