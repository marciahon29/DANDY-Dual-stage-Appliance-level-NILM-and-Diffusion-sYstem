{"cells":[{"cell_type":"code","source":["##### ANOMALY DETECTION WITH AUTOENCODER #####\n","#\n","# Density-based clustering that finds arbitrary shapes and outliers.\n","# DBSCAN groups by density, not distance or counts\n","#\n","# *cluster is the same as *interval"],"metadata":{"id":"lZUy7s7YCwcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hGwRo5amM_LD"},"outputs":[],"source":["# Maybe have to do this\n","!pip uninstall -y sympy\n","!pip install sympy==1.12\n","import sympy, importlib; importlib.reload(sympy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKJwGoL5MAKL"},"outputs":[],"source":["import os, glob, time, tracemalloc, warnings, gc\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from numpy.lib.stride_tricks import sliding_window_view\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# -----------------------------\n","# CONFIG - PATHS and PARAMETERS\n","# -----------------------------\n","RESIDENCES = [\n","    \"REFIT_House01\",\n","    \"REFIT_House02\",\"REFIT_House03\",\"REFIT_House05\",\n","    \"REFIT_House07\",\"REFIT_House09\",\"REFIT_House15\",\n","    \"UKDALE_House01\",\"UKDALE_House02\",\"UKDALE_House05\",\n","    \"AMPds2_House01\",\n","    \"GREEND_House00\",\"GREEND_House01\",\"GREEND_House03\"\n","]\n","\n","INPUT_TRAIN_PATTERN = \"/content/drive/MyDrive/Paper02_14Datasets/MERGED/{residence}_Fridge_15minutes_StepChange_MERGED.csv\"\n","INPUT_ALL_PATTERN   = \"/content/drive/MyDrive/Paper02_14Datasets/MERGED/{residence}*.csv\"\n","\n","OUTPUT_DIR_PRED     = \"/content/drive/MyDrive/Paper02_14Datasets/ANOMALY_AE\"   # keep same folder for continuity\n","OUTPUT_DIR_SUMMARY  = os.path.join(OUTPUT_DIR_PRED, \"Percentiles_Summary\")\n","MODEL_NAME          = \"AE\"  # <- changed\n","\n","# ---- Windowing / training ----\n","WIN                 = 96\n","FAST_WINDOW_STRIDE  = 4\n","INFER_WINDOW_STRIDE = 1\n","\n","# ---- AE (MLP) hyperparams ----\n","AE_HIDDEN1          = 128\n","AE_HIDDEN2          = 64\n","AE_LATENT           = 32\n","DROPOUT             = 0.1\n","\n","LR                  = 1e-3\n","EPOCHS              = 20\n","BATCH               = 256\n","EARLY_STOP          = True\n","PATIENCE            = 4\n","MIN_DELTA           = 1e-5\n","\n","# ---- Thresholding & post-processing ----\n","THR_MODE            = \"mad\"        # \"mad\" or \"percentile\"\n","THR_PCT             = 40\n","K_MAD               = 2.0\n","USE_EMA             = True\n","EMA_ALPHA           = 0.2\n","DILATE_STEPS        = 2\n","\n","USE_STANDARDIZE     = True\n","SEED                = 42\n","\n","# Simpler stable defaults\n","NUM_WORKERS         = 0\n","PIN_MEMORY          = True\n","PERSISTENT_WORKERS  = False\n","USE_AMP             = True\n","TRY_TORCH_COMPILE   = False\n","\n","# Inference batching (#windows per batch)\n","INFER_WIN_BATCH     = 100_000\n","INFER_WIN_BATCH_MIN = 20_000\n","\n","# Ensure output dirs exist\n","os.makedirs(OUTPUT_DIR_PRED, exist_ok=True)\n","os.makedirs(OUTPUT_DIR_SUMMARY, exist_ok=True)\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","rng = np.random.default_rng(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.benchmark = True\n","    try:\n","        torch.set_float32_matmul_precision(\"medium\")\n","    except Exception:\n","        pass\n","\n","# -----------------------------\n","# Helpers\n","# -----------------------------\n","# Ensure correct formatting for timestamp, active_power, and ground_truth_anomaly\n","def read_csv_safe(path: str) -> pd.DataFrame:\n","    df = pd.read_csv(path)\n","    if \"timestamp\" in df.columns:\n","        try:\n","            ts = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n","            df[\"timestamp\"] = ts.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n","        except Exception:\n","            pass\n","    if \"active_power\" in df.columns:\n","        df[\"active_power\"] = pd.to_numeric(df[\"active_power\"], errors=\"coerce\")\n","    if \"ground_truth_anomaly\" in df.columns:\n","        df[\"ground_truth_anomaly\"] = df[\"ground_truth_anomaly\"].astype(str).str.strip()\n","        df.loc[~df[\"ground_truth_anomaly\"].eq(\"Anomaly\"), \"ground_truth_anomaly\"] = \"Normal\"\n","    return df\n","\n","# Get megabyte value from bytes\n","def mb(bytes_val: int) -> float:\n","    return round(bytes_val / (1024 * 1024), 3)\n","\n","# Overwrite the CSV file\n","def save_csv_overwrite(df: pd.DataFrame, path: str):\n","    os.makedirs(os.path.dirname(path), exist_ok=True)\n","    df.to_csv(path, index=False)\n","\n","# Calculate the metrics: Total/TP/TN/FB/FN/ActualNormal/ActualAnomaly/Accuracy/Precision/Recall/F1-Score/Normal_%/Anomaly_%\n","def safe_metrics(y_true: pd.Series, y_pred: pd.Series):\n","    yt = y_true.fillna(\"Normal\").astype(str)\n","    yp = y_pred.fillna(\"Normal\").astype(str)\n","    actual_anom_mask  = yt.eq(\"Anomaly\")\n","    actual_norm_mask  = yt.eq(\"Normal\")\n","    pred_anom_mask    = yp.eq(\"Anomaly\")\n","    pred_norm_mask    = yp.eq(\"Normal\")\n","    TP = int(((actual_anom_mask) & (pred_anom_mask)).sum())\n","    TN = int(((actual_norm_mask) & (pred_norm_mask)).sum())\n","    FP = int(((actual_norm_mask) & (pred_anom_mask)).sum())\n","    FN = int(((actual_anom_mask) & (pred_norm_mask)).sum())\n","    total = int(len(yt))\n","    actual_anom = int(actual_anom_mask.sum())\n","    actual_norm = int(actual_norm_mask.sum())\n","    accuracy  = (TP + TN) / total if total else 0.0\n","    precision = TP / (TP + FP) if (TP + FP) else 0.0\n","    recall    = TP / (TP + FN) if (TP + FN) else 0.0\n","    f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n","    normal_pct  = (TN / actual_norm * 100.0) if actual_norm else 0.0\n","    anomaly_pct = (TP / actual_anom * 100.0) if actual_anom else 0.0\n","    return {\"Total\": total, \"TP\": TP, \"TN\": TN, \"FP\": FP, \"FN\": FN,\n","            \"ActualNormal\": actual_norm, \"ActualAnomaly\": actual_anom,\n","            \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1,\n","            \"Normal_%\": normal_pct, \"Anomaly_%\": anomaly_pct}\n","\n","# -----------------------------\n","# Autoencoder Architecture + Processes\n","# -----------------------------\n","class AE_MLP(nn.Module):\n","    def __init__(self, win: int, h1=128, h2=64, latent=32, dropout=0.1):\n","        super().__init__()\n","        self.win = win\n","        self.encoder = nn.Sequential(\n","            nn.Linear(win, h1), nn.ReLU(inplace=True), nn.Dropout(dropout),\n","            nn.Linear(h1, h2),  nn.ReLU(inplace=True),\n","            nn.Linear(h2, latent)\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.Linear(latent, h2), nn.ReLU(inplace=True),\n","            nn.Linear(h2, h1),     nn.ReLU(inplace=True), nn.Dropout(dropout),\n","            nn.Linear(h1, win)\n","        )\n","\n","    def forward(self, x_winB_T1: torch.Tensor) -> torch.Tensor:\n","        # x_winB_T1: [B, WIN, 1] -> flatten to [B, WIN]\n","        x = x_winB_T1.squeeze(-1)\n","        z = self.encoder(x)\n","        y = self.decoder(z)\n","        # return reconstructed as [B, WIN, 1] to match aggregation code if needed\n","        return y.unsqueeze(-1)\n","\n","# Compile if allowed.\n","def maybe_compile(model: nn.Module) -> nn.Module:\n","    if TRY_TORCH_COMPILE:\n","        try:\n","            model = torch.compile(model)\n","        except Exception:\n","            pass\n","    return model\n","\n","\n","# Windows your signal and also keeps a map back to original timestamps/rows.\n","def build_windows_fast(series: np.ndarray, win: int, stride: int):\n","    series = series.astype(np.float32, copy=False)\n","    n = len(series)\n","    if n < win:\n","        return np.empty((0, win, 1), dtype=np.float32), np.empty((0, win), dtype=np.int32), n\n","    sw = sliding_window_view(series, window_shape=win)      # [n-win+1, win] (view)\n","    if stride > 1:\n","        sw = sw[::stride]\n","    X = sw[..., None]                                       # [Nwin, win, 1] (view)\n","    starts = np.arange(0, n - win + 1, stride, dtype=np.int32)\n","    covers = starts[:, None] + np.arange(win, dtype=np.int32)[None, :]\n","    return X, covers, n\n","\n","# It runs the model on overlapping windows and averages MSE reconstruction error\n","# Computes per-timestep reconstruction error from overlapping windows safely.\n","@torch.inference_mode()\n","def pointwise_mse_batched(model: nn.Module,\n","                          X_view: np.ndarray,\n","                          covers: np.ndarray,\n","                          length: int,\n","                          device: str,\n","                          use_amp: bool = True,\n","                          win_batch: int = 100_000,\n","                          win_batch_min: int = 20_000) -> np.ndarray:\n","\n","    sums = np.zeros(length, dtype=np.float64)\n","    cnts = np.zeros(length, dtype=np.int64)\n","\n","    Nwin = X_view.shape[0]\n","    i = 0\n","    cur_bs = int(win_batch)\n","\n","    while i < Nwin:\n","        j = min(i + cur_bs, Nwin)\n","        Xb = X_view[i:j]\n","        covb = covers[i:j]\n","\n","        xt = torch.from_numpy(Xb).to(device, non_blocking=True)\n","        try:\n","            if device == \"cuda\" and use_amp:\n","                with torch.cuda.amp.autocast():\n","                    rt = model(xt)\n","            else:\n","                rt = model(xt)\n","\n","            ### point-wise squared error within each window\n","            err = (xt - rt).pow(2).squeeze(-1).float().cpu().numpy()\n","\n","            np.add.at(sums, covb.ravel(), err.ravel())\n","            np.add.at(cnts, covb.ravel(), 1)\n","\n","            i = j\n","            del xt, rt, err, covb, Xb\n","            if device == \"cuda\":\n","                torch.cuda.empty_cache()\n","        except RuntimeError as e:\n","            msg = str(e).lower()\n","            if (\"out of memory\" in msg or \"cuda\" in msg) and cur_bs > win_batch_min:\n","                cur_bs = max(win_batch_min, cur_bs // 2)\n","                if device == \"cuda\":\n","                    torch.cuda.empty_cache()\n","                gc.collect()\n","                print(f\"[OOM-avoid] reduce window-batch to {cur_bs}\")\n","            else:\n","                raise\n","\n","        if (i % max(win_batch_min, 1)) == 0:\n","            gc.collect()\n","\n","    cnts = np.maximum(cnts, 1)\n","\n","    # point-wise (per-timestep) mean squared error\n","    return (sums / cnts).astype(np.float64)\n","\n","# Performs z-score standardization on training data\n","def standardize_train_then_apply(train_vals: np.ndarray, test_vals: np.ndarray):\n","    mu = np.nanmean(train_vals)\n","    sd = np.nanstd(train_vals)\n","    if not np.isfinite(sd) or sd == 0.0:\n","        sd = 1e-9\n","    tr = (train_vals - mu) / sd\n","    te = (test_vals  - mu) / sd\n","    return tr.astype(np.float32), te.astype(np.float32), (mu, sd)\n","\n","# -----------------------------\n","# Error smoothing & thresholding\n","# A time point is labeled Anomaly if its reconstruction error (point-wise MSE)\n","# is greater than the MAD-based threshold.\n","# -----------------------------\n","# Smooths error using exponential moving average\n","def ema_1d(x: np.ndarray, alpha: float) -> np.ndarray:\n","    x = np.asarray(x, dtype=np.float64)\n","    if not np.isfinite(x).any():\n","        return np.zeros_like(x, dtype=np.float64)\n","    x = np.nan_to_num(x, nan=0.0, posinf=np.nanmax(x[np.isfinite(x)]), neginf=0.0)\n","    y = np.empty_like(x, dtype=np.float64)\n","    s = 0.0\n","    a = float(alpha)\n","    for i, v in enumerate(x):\n","        s = a * v + (1.0 - a) * (s if i > 0 else v)\n","        y[i] = s\n","    return y\n","\n","# Computes robust threshold using median deviation\n","# A robust threshold is a cutoff that is not easily distorted by outliers or extreme values\n","def mad_threshold(err: np.ndarray, k: float) -> float:\n","    med = np.nanmedian(err)\n","    mad = np.nanmedian(np.abs(err - med))\n","    if not np.isfinite(mad) or mad == 0.0:\n","        mad = 1e-9\n","    return float(med + k * mad)\n","\n","# Learns anomaly threshold from training errors - either MAD / percentile based\n","# MAD: Median Absolute Deviation, a robust alternative to mean ± std\n","def learn_threshold(train_point_err: np.ndarray) -> float:\n","    te = ema_1d(train_point_err, EMA_ALPHA) if USE_EMA else train_point_err\n","    if THR_MODE.lower() == \"mad\":\n","        return mad_threshold(te, K_MAD)\n","    else:\n","        return float(np.nanpercentile(te, THR_PCT))\n","\n","# Applies threshold, outputs anomaly labels as \"Anomal\" or \"Normal\"\n","def apply_threshold(point_err: np.ndarray, thr: float) -> np.ndarray:\n","    pe = ema_1d(point_err, EMA_ALPHA) if USE_EMA else point_err\n","    labels = (pe > thr).astype(np.int8)\n","    if DILATE_STEPS > 0:\n","        k = 2 * DILATE_STEPS + 1\n","        kernel = np.ones(k, dtype=np.int8)\n","        conv = np.convolve(labels, kernel, mode=\"same\")\n","        labels = (conv > 0).astype(np.int8)\n","    return np.where(labels == 1, \"Anomaly\", \"Normal\")\n","\n","# -----------------------------\n","# Main loop per residence\n","# -----------------------------\n","for residence in RESIDENCES:\n","    print(f\"\\n==== Processing {residence} (Model={MODEL_NAME}, THR_MODE={THR_MODE}, STRIDE(TR/TE)={FAST_WINDOW_STRIDE}/{INFER_WINDOW_STRIDE}) ====\")\n","\n","    # -----------------------------\n","    # TRAINING PROCESS\n","    # -----------------------------\n","    # Get appropriate data\n","    train_path = INPUT_TRAIN_PATTERN.format(residence=residence)\n","    if not os.path.exists(train_path):\n","        print(f\"[WARN] Training file not found: {train_path}. Skipping residence.\")\n","        continue\n","\n","    df_train_full = read_csv_safe(train_path)\n","    if \"timestamp\" in df_train_full.columns:\n","        try:\n","            df_train_full[\"_ts_sort\"] = pd.to_datetime(df_train_full[\"timestamp\"], errors=\"coerce\")\n","            df_train_full = df_train_full.sort_values(\"_ts_sort\").drop(columns=[\"_ts_sort\"])\n","        except Exception:\n","            pass\n","\n","    n = len(df_train_full)\n","    if n < WIN:\n","        print(f\"[WARN] Not enough samples ({n}) for window={WIN}. Skipping residence.\")\n","        continue\n","\n","    # Determine 80% split\n","    split_idx = int(n * 0.8)\n","    s_train_full = pd.to_numeric(df_train_full[\"active_power\"], errors=\"coerce\").replace([np.inf, -np.inf], np.nan).fillna(0.0).values\n","    s_train = s_train_full[:split_idx]\n","\n","    # Standardize data and create windows\n","    if USE_STANDARDIZE:\n","        s_train_std, _, (mu_ap, sd_ap) = standardize_train_then_apply(s_train, s_train)\n","        s_model_train = s_train_std\n","    else:\n","        mu_ap, sd_ap = 0.0, 1.0\n","        s_model_train = s_train.astype(np.float32)\n","\n","    # Vectorized training windows (stride=4)\n","    Xtr_np, covers_tr, len_tr = build_windows_fast(s_model_train, WIN, FAST_WINDOW_STRIDE)\n","    if Xtr_np.shape[0] == 0:\n","        print(f\"[WARN] Windowing returned 0 windows. Skipping residence.\")\n","        continue\n","\n","    ds = TensorDataset(torch.from_numpy(Xtr_np))  # stores [N, WIN, 1]\n","    dl = DataLoader(\n","        ds, batch_size=BATCH, shuffle=True, drop_last=False,\n","        num_workers=NUM_WORKERS, pin_memory=(PIN_MEMORY and DEVICE==\"cuda\"),\n","        persistent_workers=PERSISTENT_WORKERS\n","    )\n","\n","    # Create the Autoencoder model and compile\n","    model = AE_MLP(WIN, AE_HIDDEN1, AE_HIDDEN2, AE_LATENT, DROPOUT).to(DEVICE)\n","    model = maybe_compile(model)\n","\n","    fused_ok = (DEVICE == \"cuda\")\n","    try:\n","        opt = torch.optim.AdamW(model.parameters(), lr=LR, fused=fused_ok)\n","    except TypeError:\n","        opt = torch.optim.AdamW(model.parameters(), lr=LR)\n","    loss_fn = nn.MSELoss()\n","\n","    best_loss = float(\"inf\"); bad = 0\n","    scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and DEVICE==\"cuda\"))\n","\n","    tracemalloc.start()\n","    t0 = time.perf_counter()\n","\n","    model.train()\n","    # Training Loop\n","    for epoch in range(1, EPOCHS + 1):\n","        epoch_loss = 0.0\n","        for (xb,) in dl:  # xb: [B, WIN, 1]\n","            xb = xb.to(DEVICE, non_blocking=True)\n","            opt.zero_grad(set_to_none=True)\n","            if USE_AMP and DEVICE == \"cuda\":\n","                with torch.cuda.amp.autocast():\n","                    recon = model(xb)         # [B, WIN, 1]\n","                    loss  = loss_fn(recon, xb)\n","                scaler.scale(loss).backward()\n","                scaler.step(opt)\n","                scaler.update()\n","            else:\n","                recon = model(xb)\n","                loss  = loss_fn(recon, xb)\n","                loss.backward()\n","                opt.step()\n","            epoch_loss += loss.item() * xb.size(0)\n","\n","        epoch_loss /= len(ds)\n","        if epoch % max(1, EPOCHS // 5) == 0:\n","            print(f\"  Epoch {epoch}/{EPOCHS}  |  MSE={epoch_loss:.6f}\")\n","\n","        if EARLY_STOP:\n","            if epoch_loss + MIN_DELTA < best_loss:\n","                best_loss = epoch_loss; bad = 0\n","            else:\n","                bad += 1\n","                if bad >= PATIENCE:\n","                    print(f\"  Early stopping at epoch {epoch} (best MSE={best_loss:.6f})\")\n","                    break\n","\n","    train_time_sec = time.perf_counter() - t0\n","    train_current, train_peak = tracemalloc.get_traced_memory()\n","    tracemalloc.stop()\n","    train_peak_mb = mb(train_peak)\n","\n","    #### This block learns an anomaly threshold from training data, using the model’s reconstruction error.\n","    model.eval()\n","    with torch.no_grad():\n","        xtr = torch.from_numpy(Xtr_np).to(DEVICE)\n","        if USE_AMP and DEVICE == \"cuda\":\n","            with torch.cuda.amp.autocast():\n","                rtr = model(xtr)\n","        else:\n","            rtr = model(xtr)\n","\n","    # aggregate MSE per index\n","    def pointwise_mse_from_windows_fast(x_windows: torch.Tensor, x_recon: torch.Tensor, covers: np.ndarray, length: int):\n","        err = (x_windows - x_recon).pow(2).squeeze(-1).float().cpu().numpy()\n","        sums = np.zeros(length, dtype=np.float64); cnts = np.zeros(length, dtype=np.int64)\n","        np.add.at(sums, covers.ravel(), err.ravel())\n","        np.add.at(cnts, covers.ravel(), 1)\n","        cnts = np.maximum(cnts, 1)\n","        return (sums / cnts).astype(np.float64)\n","\n","    train_point_err = pointwise_mse_from_windows_fast(xtr, rtr, covers_tr, length=len_tr)\n","\n","    # Threshold is learned\n","    thr = learn_threshold(train_point_err)\n","    print(f\" Trained {MODEL_NAME}: train_point_err[{len(train_point_err)}], threshold={thr:.6e}\")\n","    print(f\" TrainingTimeSec={train_time_sec:.3f}, TrainPeakMB={train_peak_mb}\")\n","\n","    # -----------------------------\n","    # INFERENCE PROCESS\n","    # -----------------------------\n","    # Get the files for the residence\n","    pattern = INPUT_ALL_PATTERN.format(residence=residence)\n","    all_files = sorted(glob.glob(pattern))\n","    if not all_files:\n","        print(f\"[WARN] No files found for pattern: {pattern}\")\n","        continue\n","\n","    summary_rows = []\n","    tracemalloc.start()\n","    t_inf0 = time.perf_counter()\n","\n","    for in_path in all_files:\n","        try:\n","            base = os.path.basename(in_path)\n","            out_path = os.path.join(OUTPUT_DIR_PRED, f\"{os.path.splitext(base)[0]}_{MODEL_NAME}.csv\")\n","\n","            # Loads clean active_power to the right format\n","            df = read_csv_safe(in_path).copy()\n","            ap = pd.to_numeric(df[\"active_power\"], errors=\"coerce\").replace([np.inf, -np.inf], np.nan).fillna(0.0).values\n","\n","            # Skips files that are too short\n","            if len(ap) < WIN:\n","                df[\"prediction_anomaly\"] = \"Normal\"\n","                save_csv_overwrite(df, out_path)\n","                if \"ground_truth_anomaly\" in df.columns:\n","                    m = safe_metrics(df[\"ground_truth_anomaly\"], df[\"prediction_anomaly\"])\n","                else:\n","                    m = {\"Total\": len(df), \"TP\":0,\"TN\":0,\"FP\":0,\"FN\":0,\n","                         \"ActualNormal\":0,\"ActualAnomaly\":0,\n","                         \"Accuracy\":np.nan,\"Precision\":np.nan,\"Recall\":np.nan,\"F1-Score\":np.nan,\n","                         \"Normal_%\":np.nan,\"Anomaly_%\":np.nan}\n","                summary_rows.append({\n","                    \"Filename\": base, \"Accuracy\": m.get(\"Accuracy\", np.nan),\n","                    \"Precision\": m.get(\"Precision\", np.nan), \"Recall\": m.get(\"Recall\", np.nan),\n","                    \"F1-Score\": m.get(\"F1-Score\", np.nan), \"Normal_%\": m.get(\"Normal_%\", np.nan),\n","                    \"Anomaly_%\": m.get(\"Anomaly_%\", np.nan), \"Total\": m.get(\"Total\", 0),\n","                    \"TP\": m.get(\"TP\",0), \"TN\": m.get(\"TN\",0), \"FP\": m.get(\"FP\",0), \"FN\": m.get(\"FN\",0),\n","                    \"ActualNormal\": m.get(\"ActualNormal\",0), \"ActualAnomaly\": m.get(\"ActualAnomaly\",0),\n","                    \"TrainingTimeSec\": None, \"InferenceTimeSec\": None,\n","                    \"TrainPeakMB\": train_peak_mb, \"InferencePeakMB\": None,\n","                })\n","                print(f\"  [SKIP short] {base} (len<{WIN})\")\n","                continue\n","\n","            # Standardize using TRAIN stats\n","            s_te = ((ap - mu_ap) / (sd_ap if sd_ap != 0 else 1e-9)).astype(np.float32) if USE_STANDARDIZE else ap.astype(np.float32)\n","\n","            # Build Windows\n","            Xte_view, covers_te, len_te = build_windows_fast(s_te, WIN, INFER_WINDOW_STRIDE)\n","\n","            # Run the trained autoencoder\n","            point_err = pointwise_mse_batched(\n","                model, Xte_view, covers_te, length=len_te, device=DEVICE,\n","                use_amp=(USE_AMP and DEVICE==\"cuda\"),\n","                win_batch=INFER_WIN_BATCH, win_batch_min=INFER_WIN_BATCH_MIN\n","            )\n","\n","            # Apply the thresholds and save\n","            pred_labels = apply_threshold(point_err, thr)\n","            df[\"prediction_anomaly\"] = pd.Series(pred_labels, index=df.index).astype(str)\n","            save_csv_overwrite(df, out_path)\n","\n","            if \"ground_truth_anomaly\" in df.columns:\n","                m = safe_metrics(df[\"ground_truth_anomaly\"], df[\"prediction_anomaly\"])\n","            else:\n","                m = {\"Total\": len(df), \"TP\":0,\"TN\":0,\"FP\":0,\"FN\":0,\n","                     \"ActualNormal\":0,\"ActualAnomaly\":0,\n","                     \"Accuracy\":np.nan,\"Precision\":np.nan,\"Recall\":np.nan,\"F1-Score\":np.nan,\n","                     \"Normal_%\":np.nan,\"Anomaly_%\":np.nan}\n","\n","            summary_rows.append({\n","                \"Filename\": base,\n","                \"Accuracy\": m[\"Accuracy\"],\n","                \"Precision\": m[\"Precision\"],\n","                \"Recall\": m[\"Recall\"],\n","                \"F1-Score\": m[\"F1-Score\"],\n","                \"Normal_%\": m[\"Normal_%\"],\n","                \"Anomaly_%\": m[\"Anomaly_%\"],\n","                \"Total\": m[\"Total\"],\n","                \"TP\": m[\"TP\"], \"TN\": m[\"TN\"], \"FP\": m[\"FP\"], \"FN\": m[\"FN\"],\n","                \"ActualNormal\": m[\"ActualNormal\"], \"ActualAnomaly\": m[\"ActualAnomaly\"],\n","                \"TrainingTimeSec\": None,\n","                \"InferenceTimeSec\": None,\n","                \"TrainPeakMB\": train_peak_mb,\n","                \"InferencePeakMB\": None,\n","            })\n","\n","            print(f\"  ✔ {base}  |  Acc={m['Accuracy']:.4f}  N%={m['Normal_%']:.2f}  A%={m['Anomaly_%']:.2f}\")\n","\n","            if DEVICE == \"cuda\":\n","                torch.cuda.empty_cache()\n","            gc.collect()\n","\n","        except Exception as e:\n","            print(f\"  [ERROR] {in_path}: {e}\")\n","\n","    inference_time_sec = time.perf_counter() - t_inf0\n","    inf_current, inf_peak = tracemalloc.get_traced_memory()\n","    tracemalloc.stop()\n","    inference_peak_mb = mb(inf_peak)\n","\n","    for row in summary_rows:\n","        row[\"TrainingTimeSec\"] = round(train_time_sec, 6)\n","        row[\"InferenceTimeSec\"] = round(inference_time_sec, 6)\n","        row[\"InferencePeakMB\"] = inference_peak_mb\n","\n","    summary_df = pd.DataFrame(summary_rows)\n","    col_order = [\n","        \"Filename\",\n","        \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\",\n","        \"Normal_%\", \"Anomaly_%\", \"Total\",\n","        \"TP\", \"TN\", \"FP\", \"FN\", \"ActualNormal\", \"ActualAnomaly\",\n","        \"TrainingTimeSec\", \"InferenceTimeSec\", \"TrainPeakMB\", \"InferencePeakMB\"\n","    ]\n","    summary_df = summary_df[col_order]\n","    summary_path = os.path.join(OUTPUT_DIR_SUMMARY, f\"{residence}_ANOMALY_{MODEL_NAME}_OUTLINE.csv\")\n","    save_csv_overwrite(summary_df, summary_path)\n","\n","    print(f\" InferenceTimeSec={inference_time_sec:.3f}, InferencePeakMB={inference_peak_mb}\")\n","    print(f\" ✔ Summary saved: {summary_path}\")\n","\n","print(\"\\nDone.\")\n"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1autBeGkI3AKNgDP5WiH02LIDo6k2Qcq2","authorship_tag":"ABX9TyN57JZd0Ml1xMI1ATLwGXQF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}