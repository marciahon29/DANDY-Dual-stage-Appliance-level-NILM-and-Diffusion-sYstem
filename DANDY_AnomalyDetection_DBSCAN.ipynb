{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1ZcCcHdRgFV1zeVnOrCWT-Z9DWq8MN3Jr","authorship_tag":"ABX9TyMu3apiVF1lORi9W5lKLIRT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["##### ANOMALY DETECTION WITH DBSCAN #####\n","#\n","# Density-based clustering that finds arbitrary shapes and outliers.\n","# DBSCAN groups by density, not distance or counts\n","#\n","# *cluster is the same as *interval\n","#\n","# Main Idea\n","# 1. After sorting the code computes the difference between each neighboring pair.\n","# 2. If that difference is less than eps the points are in the normal region.\n","# 3. If a region contains less than mean_samples, it is normal.\n","# 4. Otherwise, it is anomaly."],"metadata":{"id":"QEDctUvN9P7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import glob\n","import time\n","import warnings\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","\n","try:\n","    import psutil\n","    HAVE_PSUTIL = True\n","except ImportError:\n","    HAVE_PSUTIL = False\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# -----------------------------\n","# Config - Parameters and Paths\n","# -----------------------------\n","BASE = \"/content/drive/MyDrive/Paper02_14Datasets\"\n","MERGED_DIR = f\"{BASE}/MERGED\"\n","OUT_DIR = f\"{BASE}/ANOMALY_DBSCAN\"\n","SUMMARY_DIR = f\"{OUT_DIR}/Percentiles_Summary\"\n","Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n","Path(SUMMARY_DIR).mkdir(parents=True, exist_ok=True)\n","\n","RESIDENCES = [\n","    \"REFIT_House01\",\n","    \"REFIT_House02\",\n","    \"REFIT_House03\",\n","    \"REFIT_House05\",\n","    \"REFIT_House07\",\n","    \"REFIT_House09\",\n","    \"REFIT_House15\",\n","    \"UKDALE_House01\",\n","    \"UKDALE_House02\",\n","    \"UKDALE_House05\",\n","    \"AMPds2_House01\",\n","    \"GREEND_House00\",\n","    \"GREEND_House01\",\n","    \"GREEND_House03\"\n","]\n","\n","DBSCAN_EPS = 0.5\n","DBSCAN_MIN_SAMPLES = 5\n","\n","# -----------------------------\n","# Helper Functions\n","# -----------------------------\n","# Measures current process memory usage in megabytes.\n","def rss_mb():\n","    if HAVE_PSUTIL:\n","        try:\n","            return psutil.Process().memory_info().rss / (1024*1024)\n","        except Exception:\n","            return None\n","    return None\n","\n","# Enforces formats and cleans the data\n","def normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.copy()\n","    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n","    ap = pd.to_numeric(df[\"active_power\"], errors=\"coerce\")\n","    ap = ap.fillna(method=\"ffill\").fillna(method=\"bfill\").astype(np.float32)\n","    df[\"active_power\"] = ap\n","    return df.dropna(subset=[\"active_power\"])\n","\n","# -----------------------------\n","# Lightweight 1D DBSCAN on z-scored values\n","#\n","# Standardizes training values into z-space\n","# Raw values are scaled to zero mean and unit variance so distances are comparable.\n","# Sorts z-values, splits by eps (maximum allowed gap between neighboring values)\n","# Consecutive points farther apart than eps are treated as belonging to different regions.\n","# Only dense runs with enough points are kept as valid clusters; sparse runs are ignored.\n","# Stores scaler, intervals, timing, memory\n","# -----------------------------\n","def fit_dbscan_1d(train_vals: np.ndarray, eps: float, min_samples: int):\n","    X = train_vals.reshape(-1, 1).astype(np.float32)\n","    scaler = StandardScaler()\n","    Xz = scaler.fit_transform(X).ravel()  # 1D array in z-space\n","\n","    start_t = time.perf_counter()\n","    mem_before = rss_mb()\n","\n","    # Sort z-values\n","    order = np.argsort(Xz)\n","    z_sorted = Xz[order]\n","\n","    # Find indices where consecutive gap > eps\n","    gaps = np.diff(z_sorted)\n","    break_idx = np.where(gaps > eps)[0]  # split after these indices\n","\n","    # list of dense regions that DBSCAN would consider clusters.\n","    intervals = []\n","    run_start = 0\n","    for b in np.append(break_idx, len(z_sorted) - 1):\n","        run_end = b  # inclusive\n","        run_len = run_end - run_start + 1\n","        if run_len >= min_samples:\n","            z_min = z_sorted[run_start]\n","            z_max = z_sorted[run_end]\n","            intervals.append((z_min, z_max))\n","        run_start = run_end + 1\n","\n","    # Merge overlapping/adjacent intervals for safety\n","    # Essentially - clean, disjoint cluster regions\n","    if intervals:\n","        intervals.sort(key=lambda t: t[0])\n","        merged = []\n","        cur_s, cur_e = intervals[0]\n","        for s, e in intervals[1:]:\n","            if s <= cur_e:  # overlap/adjacent in z-space\n","                cur_e = max(cur_e, e)\n","            else:\n","                merged.append((cur_s, cur_e))\n","                cur_s, cur_e = s, e\n","        merged.append((cur_s, cur_e))\n","        intervals = merged\n","\n","    elapsed = time.perf_counter() - start_t\n","    mem_after = rss_mb()\n","    mem_used = (mem_after - mem_before) if (mem_before is not None and mem_after is not None) else None\n","\n","    model = {\n","        \"scaler\": scaler,\n","        \"eps\": eps,\n","        \"min_samples\": min_samples,\n","        \"intervals\": intervals  # list of (z_min, z_max), sorted, non-overlapping\n","    }\n","    return model, elapsed, mem_used\n","\n","# -----------------------------\n","# Predict using learned z-intervals\n","# Scales new values using training scaler\n","# Checks whether values fall in clusters\n","# Labels outside clusters as anomalies\n","# *cluster is the same as *interval\n","#\n","# If this new value falls inside any dense region\n","# learned from training data, it’s normal; otherwise, it’s anomalous.”\n","# -----------------------------\n","def predict_dbscan_1d(model, vals: np.ndarray) -> np.ndarray:\n","    X = vals.reshape(-1, 1).astype(np.float32)\n","    z = model[\"scaler\"].transform(X).ravel()\n","\n","    # Get the clusters\n","    intervals = model[\"intervals\"]\n","    # If no clusters found\n","    if not intervals:\n","        return np.full(z.shape[0], -1, dtype=int)  # all anomalies if no clusters learned\n","\n","    # Vectorized membership check using searchsorted on interval starts\n","    starts = np.array([s for s, _ in intervals], dtype=np.float32)\n","    ends   = np.array([e for _, e in intervals], dtype=np.float32)\n","\n","    # For each z, find rightmost interval start <= z\n","    idx = np.searchsorted(starts, z, side=\"right\") - 1\n","    in_left = idx >= 0\n","\n","    # For valid idx, check z <= corresponding end\n","    # Essentially, determines if \"start ≤ z ≤ end\"\n","    in_right = np.zeros_like(in_left, dtype=bool)\n","    valid_idx = idx[in_left]\n","    in_right[in_left] = z[in_left] <= ends[valid_idx]\n","\n","    # If 1=Normal, -1=Anomaly\n","    in_any = in_left & in_right\n","    return np.where(in_any, 1, -1).astype(int)\n","\n","# -----------------------------\n","# Main\n","# -----------------------------\n","for residence in RESIDENCES:\n","    pattern = os.path.join(MERGED_DIR, f\"{residence}*.csv\")\n","    files = sorted(glob.glob(pattern))\n","    if not files:\n","        continue\n","\n","    # Train on first file (or choose a dedicated training file)\n","    train_file = files[0]\n","    df_train = normalize_df(pd.read_csv(train_file))\n","    model, train_time, mem_used = fit_dbscan_1d(\n","        df_train[\"active_power\"].values,\n","        eps=DBSCAN_EPS,\n","        min_samples=DBSCAN_MIN_SAMPLES\n","    )\n","    mem_str = f\"{mem_used:.1f} MB\" if mem_used is not None else \"N/A\"\n","    print(f\"{residence}: Train time={train_time:.2f}s, MemΔ={mem_str}, intervals={len(model['intervals'])}\")\n","\n","    # Predict all files with the trained model\n","    for f in files:\n","        df = normalize_df(pd.read_csv(f))\n","        preds = predict_dbscan_1d(model, df[\"active_power\"].values)\n","        df[\"prediction_anomaly\"] = np.where(preds == -1, \"Anomaly\", \"Normal\")\n","\n","        out_name = f\"{os.path.splitext(os.path.basename(f))[0]}_DBSCAN.csv\"\n","        df.to_csv(os.path.join(OUT_DIR, out_name), index=False)\n","        print(f\"Wrote {out_name}\")\n"],"metadata":{"id":"uAaGPkrD48hw"},"execution_count":null,"outputs":[]}]}