{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"14EznfklrkNZy-XBz72U-RVq5yRFGLZug","authorship_tag":"ABX9TyNhv+/xuB/t3wQxK/gqNZoJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["##### OUTLINE OF FILE #####\n","### Essentially, this code performs \"prediction_appliance\"\n","\n","#### NILM CLASSIFICATION - Runs the Autoencoder/CosineSimilarity/DecisionTree to determine \"prediction_appliance\" ####\n","#### CLEAN SPACES - ground_truth_appliance/prediction_appliance may have problems in spaces ####\n","\n","#### Per residence:\n","#### timestamp, active_power, ground_truth_appliance, ground_truth_anomaly, and prediction_appliance\n","#### Summary:\n","#### current_iteration, file_name, appliance, random_seed, epochs,\n","#### batch_size, accuracy, precision, recall, f1_score, auc, poc"],"metadata":{"id":"4Aj8h_t_B4vD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### NILM CLASSIFICATION ###\n","#\n","# The purpose of this script is to predict what the turned on appliances are based on active_power\n","# We find the statistics of when an appliance is present in the combination\n","#\n","# The Algorithms are as follows\n","#\n","# TRAINING ALGORITHM (per residence)\n","# 1. Fit scaler and train autoencoder\n","# 2. Encode data using trained encoder\n","# 3. Compute cosine similarity to centroids\n","# 4. Build per-appliance binary labels\n","# 5. Train decision trees once\n","# 6. Save trained models and trees\n","#\n","# INFERENCE ALGORITHM (per residence)\n","# 1. Load encoder, scaler, decision trees\n","# 2. Encode test data with encoder\n","# 3. Compute cosine similarity features\n","# 4. Predict appliances using decision trees\n","# 5. Form appliance-combination predictions\n","# 6. Save predictions and evaluation metrics"],"metadata":{"id":"LTFTB54pB4cG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ======================================================================\n","# IMPORTS\n","# ======================================================================\n","import os\n","import warnings\n","import joblib\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Input, Dense\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",")\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.tree import DecisionTreeClassifier\n","\n","warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # suppress TF C++ logs\n","\n","\n","# ======================================================================\n","# CONFIGURATION (EDIT THESE TO MATCH YOUR FOLDERS / EXPERIMENT SETUP)\n","# ======================================================================\n","\n","# Residences (houses) you will process independently.\n","# IMPORTANT: each residence trains its *own* scaler, autoencoder, and trees.\n","RESIDENCES = [\n","    \"REFIT_House01\",\n","    \"REFIT_House02\",\n","    \"REFIT_House03\",\n","    \"REFIT_House05\",\n","    \"REFIT_House07\",\n","    \"REFIT_House09\",\n","    \"REFIT_House15\",\n","    \"UKDALE_House01\",\n","    \"UKDALE_House02\",\n","    \"UKDALE_House05\",\n","    \"AMPds2_House01\",\n","    \"GREEND_House00\",\n","    \"GREEND_House01\",\n","    \"GREEND_House03\",\n","]\n","\n","# Base directory structure (your Google Drive paths)\n","BASE_DIR     = \"/content/drive/MyDrive/Paper02_14Datasets\"\n","MERGED_DIR   = f\"{BASE_DIR}/MERGED\"         # your labelled merged datasets live here\n","CENTROIDS_DIR= f\"{BASE_DIR}/CENTROIDS\"      # centroid CSVs (one per residence) live here\n","NILM_OUT_DIR = f\"{BASE_DIR}/NILM_version_20260207\"  # outputs (models/preds/metrics)\n","\n","# Ensure output folder exists\n","os.makedirs(NILM_OUT_DIR, exist_ok=True)\n","\n","# Appliances you want to detect as ON/OFF at each time step\n","# This script will train ONE binary DecisionTree per appliance.\n","APPLIANCES = [\"Fridge\", \"WashingMachine\", \"Dishwasher\"]\n","\n","# These are anomaly types used to name your MERGED files.\n","# Note: anomalies are not predicted here; they just define which input files we run.\n","ANOMALIES = [\n","    \"StepChange\", \"MultiStepChange\", \"Mirror\",\n","    \"Repeating\", \"StuckMAX\", \"StuckMIN\", \"PowerCycling\"\n","]\n","\n","# Autoencoder training hyperparams\n","EPOCHS      = 5\n","BATCH_SIZE  = 8\n","\n","# Fixed seed for reproducibility\n","RANDOM_SEED = 42\n","\n","\n","# ======================================================================\n","# TIMESTAMP PARSING HELPER IN ORDER TO HAVE CONSISTENCY ACROSS DATASETS\n","# ======================================================================\n","def _parse_timestamp_series(ts: pd.Series) -> pd.Series:\n","    # Attempt direct parsing\n","    s = pd.to_datetime(ts, errors=\"coerce\")\n","    if s.notna().mean() > 0.9:\n","        return s\n","\n","    # If numeric timestamps (epoch) try seconds then milliseconds\n","    if pd.api.types.is_numeric_dtype(ts):\n","        s = pd.to_datetime(ts, unit=\"s\", errors=\"coerce\")\n","        if s.notna().mean() > 0.9:\n","            return s\n","\n","        s = pd.to_datetime(ts, unit=\"ms\", errors=\"coerce\")\n","        if s.notna().mean() > 0.9:\n","            return s\n","\n","    # If still bad, return whatever we got (mostly NaT)\n","    return s\n","\n","\n","# ======================================================================\n","# FIRST CALENDAR MONTH SPLIT RATIO - Based on the first date\n","# ======================================================================\n","def compute_first_month_split(file_path: str) -> float:\n","    try:\n","        df = pd.read_csv(file_path, usecols=['timestamp'])\n","    except Exception as e:\n","        print(f\"Warning: could not read timestamps from {file_path}: {e}\")\n","        return 0.1  # safe fallback if file missing/bad\n","\n","    # Parse timestamps and drop NaT\n","    ts = _parse_timestamp_series(df['timestamp']).dropna()\n","    if ts.empty:\n","        print(f\"Warning: no valid timestamps in {file_path}; using default split=0.1\")\n","        return 0.1\n","\n","    start_date = ts.min()\n","    end_date   = start_date + pd.DateOffset(months=1)\n","\n","    # Count how many rows fall in the first month window\n","    first_month_count = ((ts >= start_date) & (ts < end_date)).sum()\n","    total_count       = len(ts)\n","\n","    # Convert to ratio\n","    split_ratio = float(first_month_count) / float(total_count) if total_count > 0 else 0.1\n","\n","    # Guardrails to avoid zero or full split\n","    if not np.isfinite(split_ratio) or split_ratio <= 0:\n","        split_ratio = 0.1\n","    elif split_ratio >= 1.0:\n","        split_ratio = 0.99\n","\n","    return split_ratio\n","\n","\n","def get_training_split(file_path: str) -> float:\n","    try:\n","        return compute_first_month_split(file_path)\n","    except Exception as e:\n","        print(f\"Warning: could not compute split ratio for {file_path}: {e}\")\n","        return 0.1\n","\n","\n","# ======================================================================\n","# DATA LOADING + PREPROCESSING\n","# ======================================================================\n","def load_and_preprocess_data(file_path, centroids_path):\n","    print(f\"Loading data from {file_path}...\")\n","\n","    # Columns that should exist in your MERGED files\n","    cols_to_load = ['timestamp', 'active_power', 'ground_truth_appliance', 'ground_truth_anomaly']\n","\n","    try:\n","        # Use a lambda to safely load colums\n","        df = pd.read_csv(file_path, usecols=lambda c: c in cols_to_load)\n","    except FileNotFoundError:\n","        print(f\"Warning: File not found: {file_path}. Skipping.\")\n","        return None, None, None, None, None, None\n","    except Exception as e:\n","        print(f\"Warning: Failed to read {file_path}: {e}. Skipping.\")\n","        return None, None, None, None, None, None\n","\n","    # If there are missing values, fill numeric columns with median\n","    if df.isnull().values.any():\n","        print(\"Warning: Missing values detected. Filling with column medians where applicable.\")\n","        df = df.fillna(df.median(numeric_only=True))\n","\n","    # Extract active power (active_power) and appliance labels (ground_truth_appliance)\n","    active_power = df[\"active_power\"].values.reshape(-1, 1)\n","    ground_truth_appliance = df[\"ground_truth_appliance\"].astype(str).values\n","\n","    # Normalize active power to [0,1] using MinMaxScaler\n","    scaler = MinMaxScaler(feature_range=(0, 1))\n","    active_power_normalized = scaler.fit_transform(active_power)\n","\n","    # Compute time-based split using first-month ratio\n","    split_percentage = get_training_split(file_path)\n","    split_idx = int(split_percentage * len(active_power_normalized))\n","\n","    # Split into train/test\n","    train_data = active_power_normalized[:split_idx]\n","    test_data  = active_power_normalized[split_idx:]\n","    test_ground_truth_appliance = ground_truth_appliance[split_idx:]\n","\n","    # Keep the full test dataframe (for writing predictions with timestamps)\n","    test_df = df.iloc[split_idx:].reset_index(drop=True)\n","\n","    # Load centroids (per residence)\n","    print(f\"Loading centroids from {centroids_path}...\")\n","    try:\n","        centroids_df = pd.read_csv(centroids_path)\n","    except Exception as e:\n","        print(f\"Warning: could not load centroids file {centroids_path}: {e}\")\n","        # Return without centroids if missing\n","        return train_data, test_data, test_ground_truth_appliance, None, scaler, test_df\n","\n","    # Fill centroid missing numeric values too\n","    if centroids_df.isnull().values.any():\n","        centroids_df = centroids_df.fillna(centroids_df.median(numeric_only=True))\n","\n","    return train_data, test_data, test_ground_truth_appliance, centroids_df, scaler, test_df\n","\n","\n","# ======================================================================\n","# AUTOENCODER: BUILD + TRAIN\n","# It is a simple Autoencoder with MSE reconstruction loss\n","# Train the autoencoder\n","# Save the autoencoder and the encoder\n","# ======================================================================\n","def build_and_train_autoencoder(train_data, encoding_dim=10, max_epochs=5, batch_size=8):\n","    # Set seeds for reproducible weight init + batching randomness\n","    tf.random.set_seed(RANDOM_SEED)\n","    np.random.seed(RANDOM_SEED)\n","\n","    input_dim = train_data.shape[1]  # should be 1 (active power)\n","\n","    # ----- Encoder -----\n","    input_layer = Input(shape=(input_dim,))\n","    encoded = Dense(16, activation='relu', kernel_initializer='he_uniform')(input_layer)\n","    encoded = Dense(encoding_dim, activation='relu', kernel_initializer='he_uniform')(encoded)\n","\n","    # ----- Decoder -----\n","    decoded = Dense(16, activation='relu', kernel_initializer='he_uniform')(encoded)\n","    decoded = Dense(input_dim, activation='sigmoid')(decoded)  # output in [0,1] because inputs are in [0,1]\n","\n","    # Full AE model reconstructs x\n","    autoencoder = Model(input_layer, decoded)\n","\n","    # Encoder model outputs the embedding\n","    encoder = Model(input_layer, encoded)\n","\n","    # Train with MSE reconstruction loss\n","    autoencoder.compile(optimizer='adam', loss='mse')\n","    autoencoder.summary()\n","\n","    print(\"\\nTraining autoencoder...\")\n","    autoencoder.fit(\n","        train_data, train_data,             # x -> x reconstruction\n","        epochs=max_epochs,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        validation_split=0.1,               # hold out 10% of train_data for validation\n","        verbose=1\n","    )\n","    print(\"Training complete.\")\n","    return encoder, autoencoder\n","\n","\n","# ======================================================================\n","# FEATURE BUILDERS: CENTROIDS + TEST ENCODING\n","# ======================================================================\n","def process_centroids(centroids_df, encoder, scaler):\n","    # Convert centroids into encoded space\n","    # Return the encoded centroids + centroid labels\n","    if centroids_df is None or centroids_df.empty:\n","        return np.empty((0, encoder.output_shape[-1])), np.array([])\n","\n","    centroids_active_power = centroids_df[\"active_power\"].values.reshape(-1, 1)\n","\n","    # IMPORTANT: use SAME scaler as the residence model\n","    centroids_active_power_normalized = scaler.transform(centroids_active_power)\n","\n","    # Encode centroid points\n","    centroids_encoded = encoder.predict(centroids_active_power_normalized, verbose=0)\n","\n","    return centroids_encoded, centroids_df[\"combination\"].astype(str).values\n","\n","\n","def encode_test_data(test_data, encoder):\n","    # Encode test into embedding space.\n","    if test_data is None or len(test_data) == 0:\n","        return np.empty((0, encoder.output_shape[-1]))\n","    return encoder.predict(test_data, batch_size=512, verbose=0)\n","\n","\n","# Consists of combination and appliance\n","# If appliance is in combination, then True is returned\n","def vectorized_is_appliance_in_combination(combinations, appliance):\n","    combinations_str = np.array(combinations, dtype=str)\n","    return np.char.find(combinations_str, appliance) >= 0\n","\n","\n","# ======================================================================\n","# TRAIN DECISION TREES ONCE (PER RESIDENCE)\n","# ======================================================================\n","def build_tree_training_set(residence: str, encoder, scaler, centroids_df):\n","    # Build the training dataset (X_tree, y_tree) for DecisionTrees.\n","    #\n","    #     - We want a classifier that predicts appliance ON/OFF from similarity features.\n","    #     - Similarity features are cosine similarities between:\n","    #          - encoded sample vector vs encoded centroid vectors\n","    #\n","    # Training Sequence\n","    #     - we take only the FIRST-MONTH portion (time-based training region)\n","    #       and use it to build similarity features + labels.\n","    #\n","    # X_tree - Cosine-similarity feature matrix (inputs to trees)\n","    # y_tree - Dictionary of binary labels per appliance\n","    #\n","    # Encode the centroids once\n","    centroids_encoded, _ = process_centroids(centroids_df, encoder, scaler)\n","\n","    # Collect feature matrices from each file\n","    X_list = []\n","\n","    # Collect labels for each appliance (multi-label)\n","    y_dict = {appl: [] for appl in APPLIANCES}\n","\n","    # Loop over all anomaly-type files\n","    for anomaly in ANOMALIES:\n","        for appliance_loop in APPLIANCES:\n","            fp = f\"{MERGED_DIR}/{residence}_{appliance_loop}_15minutes_{anomaly}_MERGED.csv\"\n","\n","            # Skip missing files\n","            if not os.path.exists(fp):\n","                continue\n","\n","            # Read raw (NOT normalized here) so we can apply the residence scaler\n","            try:\n","                df_raw = pd.read_csv(fp, usecols=[\n","                    'timestamp', 'active_power', 'ground_truth_appliance', 'ground_truth_anomaly'\n","                ])\n","            except Exception as e:\n","                print(f\"Warning: could not read {fp}: {e}\")\n","                continue\n","\n","            # Fill missing numeric values\n","            if df_raw.isnull().values.any():\n","                df_raw = df_raw.fillna(df_raw.median(numeric_only=True))\n","\n","            # FIRST-MONTH split index\n","            split_pct = get_training_split(fp)\n","            split_idx = int(split_pct * len(df_raw))\n","            if split_idx <= 0:\n","                continue\n","\n","            # Take only first-month portion for training\n","            raw_first_month = df_raw['active_power'].values[:split_idx].reshape(-1, 1)\n","            gt_first_month  = df_raw['ground_truth_appliance'].astype(str).values[:split_idx]\n","\n","            # Normalize using the RESIDENCE scaler (IMPORTANT)\n","            x_first_month = scaler.transform(raw_first_month)\n","\n","            # Encode in embedding space\n","            x_encoded = encoder.predict(x_first_month, batch_size=512, verbose=0)\n","\n","            # Compute cosine similarity features vs encoded centroids\n","            if centroids_encoded.size:\n","                sim = cosine_similarity(x_encoded, centroids_encoded)  # shape: (n_samples, n_centroids)\n","            else:\n","                sim = np.zeros((len(x_encoded), 0))  # no centroids -> empty features\n","\n","            X_list.append(sim)\n","\n","            # Build binary label vector per appliance\n","            for appl in APPLIANCES:\n","                # y=1 if appliance substring appears in ground_truth string\n","                y = vectorized_is_appliance_in_combination(gt_first_month, appl).astype(int)\n","                y_dict[appl].append(y)\n","\n","    # If no files existed or no features built, return empty\n","    if not X_list:\n","        return np.zeros((0, 0)), {appl: np.array([]) for appl in APPLIANCES}\n","\n","    # Stack all similarity feature blocks vertically\n","    X_tree = np.vstack(X_list)\n","\n","    # Concatenate label vectors per appliance\n","    y_tree = {\n","        appl: (np.concatenate(y_dict[appl]) if len(y_dict[appl]) else np.array([]))\n","        for appl in APPLIANCES\n","    }\n","\n","    return X_tree, y_tree\n","\n","\n","def train_decision_trees_once(X_tree, y_tree_dict):\n","    # Trains one DecisionTree per appliance\n","    # Input: similarity-to-centroids feature vector\n","    # output: 0/1 for \"this appliance is present\"\n","\n","    classifiers = {}\n","\n","    for appl in APPLIANCES:\n","        y = y_tree_dict.get(appl, np.array([]))\n","        clf = DecisionTreeClassifier(random_state=RANDOM_SEED)\n","\n","        # If no data or only one class, we cannot train a meaningful classifier\n","        if X_tree.size == 0 or y.size == 0 or len(np.unique(y)) <= 1:\n","            classifiers[appl] = None\n","        else:\n","            clf.fit(X_tree, y)\n","            classifiers[appl] = clf\n","\n","    return classifiers\n","\n","\n","def predict_with_decision_tree(similarities_matrix, classifiers, appliances):\n","    # Use the trained DecisionTrees to predict which appliances are ON\n","    #\n","    # Steps:\n","    #  1) For each appliance, run its binary classifier -> 0/1\n","    #  2) Combine all appliance predictions into a string:\n","    #     - if none: \"Nothing\"\n","    #     - else: \"Fridge + Dishwasher\" etc.\n","    if similarities_matrix.size == 0:\n","        return np.array([])\n","\n","    num_samples = similarities_matrix.shape[0]\n","\n","    # Store predictions per appliance\n","    appliance_preds = {}\n","    for appliance in appliances:\n","        clf = classifiers.get(appliance)\n","        if clf is not None:\n","            appliance_preds[appliance] = clf.predict(similarities_matrix)\n","        else:\n","            # If classifier missing, default to all zeros\n","            appliance_preds[appliance] = np.zeros(num_samples, dtype=int)\n","\n","    # Combine into final multi-label string\n","    predictions = []\n","    for i in range(num_samples):\n","        active_appliances = [a for a in appliances if appliance_preds[a][i] == 1]\n","        predictions.append(\"Nothing\" if not active_appliances else \" + \".join(active_appliances))\n","\n","    return np.array(predictions)\n","\n","\n","# ======================================================================\n","# METRICS - current_iteration, file_name, appliance, random_seed, epochs,\n","#           batch_size, accuracy, precision, recall, f1_score, auc, poc\n","# ======================================================================\n","def calculate_metrics(test_ground_truth, predictions, appliances, file_name):\n","    results = []\n","\n","    for appliance in appliances:\n","        y_true = vectorized_is_appliance_in_combination(test_ground_truth, appliance).astype(int)\n","        y_pred = vectorized_is_appliance_in_combination(predictions, appliance).astype(int)\n","\n","        # AUC needs both classes present in y_true\n","        auc = np.nan\n","        if len(np.unique(y_true)) > 1:\n","            try:\n","                auc = roc_auc_score(y_true, y_pred)\n","            except ValueError:\n","                auc = np.nan\n","\n","        results.append({\n","            'current_iteration': 1,\n","            'file_name': file_name,\n","            'appliance': appliance,\n","            'random_seed': RANDOM_SEED,\n","            'epochs': EPOCHS,\n","            'batch_size': BATCH_SIZE,\n","            'accuracy': accuracy_score(y_true, y_pred),\n","            'precision': precision_score(y_true, y_pred, zero_division=0),\n","            'recall': recall_score(y_true, y_pred, zero_division=0),\n","            'f1_score': f1_score(y_true, y_pred, zero_division=0),\n","            'auc': auc,\n","            'poc': np.mean(y_true == y_pred)  # same as accuracy for binary labels\n","        })\n","\n","    return results\n","\n","\n","# ======================================================================\n","# PER-RESIDENCE: TRAINING + INFERENCE\n","# ======================================================================\n","def run_for_residence(residence: str):\n","    #    TRAINING:\n","    #      - Train Autoencoder using TRAINING_FILE (first-month portion).\n","    #      - Using the Encoder build cosine-similarity features.\n","    #      - Train DecisionTrees ONCE (one per appliance) and save them.\n","    #\n","    #    INFERENCE:\n","    #      - Load saved AE + scaler + trees.\n","    #      - For each MERGED file:\n","    #          - take test portion only\n","    #          - compute similarities\n","    #          - predict appliance combination\n","    #          - save per-file prediction CSV\n","    #          - compute metrics and append to summary CSV\n","    #\n","    print(\"\\n\" + \"=\"*80)\n","    print(f\"Processing residence: {residence}\")\n","    print(\"=\"*80)\n","\n","    # Output file storing per-file per-appliance metrics\n","    RESULTS_FILE = f\"{NILM_OUT_DIR}/{residence}_NILM_Results.csv\"\n","\n","    # Residence-specific centroid file\n","    CENTROIDS_FILE = f\"{CENTROIDS_DIR}/{residence}_centroids.csv\"\n","\n","    # Choose ONE training file to fit the scaler + train autoencoder\n","    TRAINING_FILE = f\"{MERGED_DIR}/{residence}_Fridge_15minutes_StepChange_MERGED.csv\"\n","\n","    # Saved artifacts (per residence)\n","    SAVED_MODEL_PATH  = f\"{NILM_OUT_DIR}/{residence}_autoencoder.keras\"\n","    SAVED_SCALER_PATH = f\"{NILM_OUT_DIR}/{residence}_scaler.save\"\n","    SAVED_TREES_PATH  = f\"{NILM_OUT_DIR}/{residence}_trees.save\"\n","\n","    # ==================================================================\n","    # TRAINING\n","    # ==================================================================\n","    print(\"--- TRAINING: Autoencoder + Trees ---\")\n","\n","    # Load training file (this fits a scaler on TRAINING_FILE)\n","    train_data, _, _, centroids_df, scaler_trainfile, _ = load_and_preprocess_data(\n","        TRAINING_FILE, CENTROIDS_FILE\n","    )\n","\n","    # Guard checks: if no training data or no centroids, skip this residence\n","    if train_data is None or len(train_data) == 0:\n","        print(f\"Error: No training data loaded from {TRAINING_FILE}. Skipping residence {residence}.\")\n","        return\n","    if centroids_df is None or centroids_df.empty:\n","        print(f\"Error: No centroids loaded from {CENTROIDS_FILE}. Skipping residence {residence}.\")\n","        return\n","\n","    # Train Autoencoder (train_data already normalized by scaler_trainfile)\n","    encoder, autoencoder = build_and_train_autoencoder(\n","        train_data,\n","        max_epochs=EPOCHS,\n","        batch_size=BATCH_SIZE\n","    )\n","\n","    # Save the trained autoencoder + scaler\n","    autoencoder.save(SAVED_MODEL_PATH)\n","    joblib.dump(scaler_trainfile, SAVED_SCALER_PATH)\n","    print(f\"Autoencoder saved to {SAVED_MODEL_PATH}\")\n","    print(f\"Scaler saved to {SAVED_SCALER_PATH}\")\n","\n","    # Build training set for DecisionTrees (using first-month portions from many files)\n","    X_tree, y_tree = build_tree_training_set(\n","        residence=residence,\n","        encoder=encoder,\n","        scaler=scaler_trainfile,\n","        centroids_df=centroids_df\n","    )\n","\n","    # Train trees once and save them\n","    classifiers = train_decision_trees_once(X_tree, y_tree)\n","    joblib.dump(classifiers, SAVED_TREES_PATH)\n","    print(f\"Decision trees saved to {SAVED_TREES_PATH}\")\n","\n","    # ==================================================================\n","    # INFERENCE\n","    # ==================================================================\n","    print(\"\\n--- INFERENCE: Predict on All Files (NO FITTING) ---\")\n","\n","    # Load saved artifacts\n","    try:\n","        loaded_autoencoder = load_model(SAVED_MODEL_PATH)\n","\n","        # Rebuild encoder model:\n","        # - autoencoder.layers[2] is the second Dense in encoder path (encoding_dim output)\n","        encoder = Model(\n","            inputs=loaded_autoencoder.input,\n","            outputs=loaded_autoencoder.layers[2].output\n","        )\n","\n","        scaler      = joblib.load(SAVED_SCALER_PATH)\n","        classifiers = joblib.load(SAVED_TREES_PATH)\n","\n","    except Exception as e:\n","        print(f\"Error loading artifacts for {residence}: {e}\")\n","        return\n","\n","    # Create results file if not exists\n","    header = [\n","        'current_iteration', 'file_name', 'appliance', 'random_seed', 'epochs', 'batch_size',\n","        'accuracy', 'precision', 'recall', 'f1_score', 'auc', 'poc'\n","    ]\n","    if not os.path.exists(RESULTS_FILE):\n","        pd.DataFrame(columns=header).to_csv(RESULTS_FILE, index=False)\n","\n","    # Loop over all anomaly files and appliances (just to enumerate filenames)\n","    for anomaly in ANOMALIES:\n","        for appliance_loop in APPLIANCES:\n","\n","            file_name = f\"{MERGED_DIR}/{residence}_{appliance_loop}_15minutes_{anomaly}_MERGED.csv\"\n","            print(f\"\\n--- Processing: {file_name} ---\")\n","\n","            # Skip missing input files\n","            if not os.path.exists(file_name):\n","                print(f\"Skipping missing file: {file_name}\")\n","                continue\n","\n","            # Load test portion from file (but DO NOT use its own scaler for encoding)\n","            _, test_data, test_ground_truth, centroids_df, _, test_df_full = load_and_preprocess_data(\n","                file_name, CENTROIDS_FILE\n","            )\n","            if test_data is None or len(test_data) == 0:\n","                print(f\"Skipping {file_name}: no test data.\")\n","                continue\n","\n","            # Encode centroids + test data using residence encoder + residence scaler\n","            centroids_encoded, _ = process_centroids(centroids_df, encoder, scaler)\n","            test_encoded = encode_test_data(test_data, encoder)\n","\n","            # Build similarity feature matrix (test samples x centroid count)\n","            if centroids_encoded.size:\n","                similarities_matrix = cosine_similarity(test_encoded, centroids_encoded)\n","            else:\n","                similarities_matrix = np.zeros((len(test_encoded), 0))\n","\n","            # PREDICT ONLY (NO TRAINING HERE)\n","            predictions = predict_with_decision_tree(\n","                similarities_matrix,\n","                classifiers,\n","                APPLIANCES\n","            )\n","\n","            # Save per-file predictions CSV with timestamp + GT + prediction\n","            output_df = test_df_full[['timestamp', 'active_power', 'ground_truth_appliance', 'ground_truth_anomaly']].copy()\n","            output_df['prediction_appliance'] = predictions\n","\n","            output_filename = f\"{NILM_OUT_DIR}/{residence}_{appliance_loop}_15minutes_{anomaly}_NILM.csv\"\n","            output_df.to_csv(output_filename, index=False)\n","            print(f\"Predictions saved to {output_filename}\")\n","\n","            # Compute and append metrics\n","            iteration_results = calculate_metrics(\n","                test_ground_truth,\n","                predictions,\n","                APPLIANCES,\n","                file_name\n","            )\n","            results_df = pd.DataFrame(iteration_results)\n","            results_df.to_csv(RESULTS_FILE, mode='a', header=False, index=False)\n","            print(f\"Metrics appended to {RESULTS_FILE}\")\n","\n","            # Free TF graph memory between loops (helps in Colab)\n","            tf.keras.backend.clear_session()\n","\n","    print(f\"\\n--- Completed residence: {residence} ---\")\n","\n","\n","# ======================================================================\n","# MAIN EXECUTION\n","# ======================================================================\n","def main():\n","    \"\"\"\n","    Runs the full pipeline for all residences.\n","    \"\"\"\n","    for residence in RESIDENCES:\n","        run_for_residence(residence)\n","\n","    print(\"\\n\\n=== All residences processed. ===\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"lChsVvpGe6o_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BcaIym7qe6kA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### CLEAN SPACES - ground_truth_appliance/prediction_appliance may have problems in spaces ####\n","#### This code removes \" \" with \"\" (white spaces) in the fields ground_truth_appliance and prediction_appliance ####\n","import os\n","import glob\n","import pandas as pd\n","\n","# Path to NILM directory\n","NILM_DIR = \"/content/drive/MyDrive/Paper02_14Datasets/NILM_version_20260207\"\n","\n","# Find all CSV files in the directory\n","csv_files = glob.glob(os.path.join(NILM_DIR, \"*.csv\"))\n","\n","for file in csv_files:\n","    try:\n","        df = pd.read_csv(file)\n","\n","        # Clean columns if they exist\n","        if \"ground_truth_appliance\" in df.columns:\n","            df[\"ground_truth_appliance\"] = df[\"ground_truth_appliance\"].astype(str).str.replace(\" \", \"\")\n","        if \"prediction_appliance\" in df.columns:\n","            df[\"prediction_appliance\"] = df[\"prediction_appliance\"].astype(str).str.replace(\" \", \"\")\n","\n","        # Save cleaned file (overwrite original)\n","        df.to_csv(file, index=False)\n","        print(f\"✅ Cleaned and saved: {os.path.basename(file)}\")\n","\n","    except Exception as e:\n","        print(f\"⚠️ Skipped {os.path.basename(file)} due to error: {e}\")\n"],"metadata":{"id":"m4rXpCzNUEHX"},"execution_count":null,"outputs":[]}]}