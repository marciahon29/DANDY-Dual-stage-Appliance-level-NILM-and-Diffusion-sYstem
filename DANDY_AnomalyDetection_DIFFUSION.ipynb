{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1NZHIP8ionDOEMKB_4M4VGCUCwpONzIrw","authorship_tag":"ABX9TyNi4p2AhGqCLswlQslnf1W0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["##### ANOMALY DETECTION WITH DIFFUSION #####\n","#\n","#"],"metadata":{"id":"BM3RV4kpUwxF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsQxZWwdUih7"},"outputs":[],"source":["# -----------------------------\n","# Config - Path / Parameters\n","# -----------------------------\n","import os, json, time, gc, tracemalloc, warnings, logging\n","import numpy as np, pandas as pd, torch\n","from diffusers import DDPMScheduler, UNet1DModel\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"diffusers\")\n","logging.getLogger(\"diffusers\").setLevel(logging.ERROR)\n","\n","BASE = \"/content/drive/MyDrive/Paper02_14Datasets\"\n","MERGED_DIR = f\"{BASE}/MERGED\"\n","OUT_DIR = f\"{BASE}/ANOMALY_DIFFUSION_RESIDUALSPECTRAL\"\n","SUMMARY_DIR = f\"{OUT_DIR}/Percentiles_Summary\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","os.makedirs(SUMMARY_DIR, exist_ok=True)\n","\n","TIME_COL, VAL_COL, FREQ = \"timestamp\", \"active_power\", \"15min\"\n","WIN = 24 * 4  # 96 points (15-min grid per day)\n","STEPS, LR, TSTEPS, BATCH, T_FIX = 600, 1e-4, 1000, 32, 500\n","THR_PCT = 70\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","RESIDENCES = [\n","    \"AMPds2_House01\",\n","    \"GREEND_House00\", \"GREEND_House01\", \"GREEND_House03\",\n","    \"UKDALE_House01\", \"UKDALE_House02\", \"UKDALE_House05\",\n","    \"REFIT_House01\", \"REFIT_House02\", \"REFIT_House03\", \"REFIT_House05\",\n","    \"REFIT_House07\", \"REFIT_House09\", \"REFIT_House15\"\n","    ]\n","APPLIANCES = [\"Fridge\", \"WashingMachine\", \"Dishwasher\"]\n","ANOMALIES = [\n","    \"StepChange\",\n","    \"MultiStepChange\",\n","    \"Repeating\",\n","    \"Mirror\",\n","    \"StuckMAX\",\n","    \"StuckMIN\",\n","    \"PowerCycling\",\n","]\n","\n","# -----------------------------\n","# HELPERS\n","# -----------------------------\n","# Loads CSV, cleans timestamps, resamples to 15-minutes\n","def load_series_15T(csv_path):\n","    df = pd.read_csv(csv_path, dtype={TIME_COL: str})\n","    df[TIME_COL] = pd.to_datetime(df[TIME_COL].str.strip(), errors=\"coerce\")\n","    df = df.dropna(subset=[TIME_COL])\n","    s = (\n","        pd.to_numeric(df[VAL_COL], errors=\"coerce\")\n","        .rename(\"v\")\n","        .to_frame()\n","        .set_index(df[TIME_COL])\n","        .sort_index()[\"v\"]\n","    )\n","    return s.resample(FREQ).interpolate(\"time\").ffill().bfill()\n","\n","\n","# Splits series into complete 96-point days (96x15minutes = 24hours)\n","def daily_windows(series):\n","    if series.empty:\n","        return np.empty((0, WIN), \"float32\"), pd.Index([]), pd.Index([])\n","    start = (\n","        series.index[0].normalize()\n","        if series.index[0].time() == pd.Timestamp.min.time()\n","        else (series.index[0] + pd.Timedelta(days=1)).normalize()\n","    )\n","    days = pd.date_range(start, series.index[-1].normalize(), freq=\"D\")\n","    X, S, E = [], [], []\n","    for d in days:\n","        idx = pd.date_range(d, d + pd.Timedelta(hours=23, minutes=45), freq=\"15min\")\n","        seg = series.reindex(idx).interpolate(\"time\").ffill().bfill()\n","        if len(seg) == WIN:\n","            X.append(seg.values.astype(\"float32\"))\n","            S.append(idx[0])\n","            E.append(idx[-1])\n","    if not X:\n","        return np.empty((0, WIN), \"float32\"), pd.Index([]), pd.Index([])\n","    return np.stack(X), pd.Index(S), pd.Index(E)\n","\n","# Do z-standardization\n","def standardize(Xtr, Xte):\n","    mu, sd = Xtr.mean(0, keepdims=True), Xtr.std(0, keepdims=True) + 1e-12\n","    return (Xtr - mu) / sd, (Xte - mu) / sd, mu, sd\n","\n","# Build a simple architecture using the libraries: UNet1DModel with a DDPMScheduler\n","def build_model():\n","    net = UNet1DModel(\n","        sample_size=WIN,\n","        in_channels=1,\n","        out_channels=1,\n","        layers_per_block=2,\n","        block_out_channels=(64,),\n","        down_block_types=(\"DownBlock1D\",),\n","        up_block_types=(\"UpBlock1D\",),\n","    ).to(DEVICE)\n","    sch = DDPMScheduler(num_train_timesteps=TSTEPS)\n","    opt = torch.optim.AdamW(net.parameters(), lr=LR)\n","    return net, sch, opt\n","\n","# Robust z-standardization is z-scaling that ignores outliers.\n","def robust_z(x):\n","    x = np.asarray(x, \"float32\")\n","    med = np.median(x)\n","    mad = np.median(np.abs(x - med)) + 1e-12\n","    return (x - med) / (1.4826 * mad)\n","\n","\n","# Determines the Reconstruction Error\n","# Adds fixed diffusion noise and denoises once\n","# Reconstructs clean signal estimate\n","# Computes time-domain MAE and frequency-domain MAE\n","# Returns errors the MAE's and original signal variance\n","@torch.no_grad()\n","def recon_err(net, sch, Xt):\n","    if Xt.shape[0] == 0:\n","        Z = np.array([], \"float32\")\n","        return Z, Z, Z\n","    B = Xt.shape[0]\n","    tvec = torch.full((B,), T_FIX, dtype=torch.int64, device=DEVICE)\n","    ts = torch.tensor(T_FIX, dtype=torch.int64, device=DEVICE)\n","    noisy = sch.add_noise(Xt, torch.randn_like(Xt), tvec)\n","    eps = net(noisy, tvec).sample\n","    out = sch.step(eps, ts, noisy)\n","    if getattr(out, \"pred_original_sample\", None) is not None:\n","        x0 = out.pred_original_sample\n","    else:\n","        ac = sch.alphas_cumprod.to(Xt.device)\n","        at = ac[ts]\n","        x0 = (noisy - torch.sqrt(1 - at).view(1, 1, 1) * eps) / (\n","            torch.sqrt(at).view(1, 1, 1) + 1e-12\n","        )\n","    mae_t = torch.mean(torch.abs(Xt - x0), dim=(1, 2))\n","    def _logmag(u): return torch.log1p(torch.abs(torch.fft.rfft(u, dim=-1)))\n","    mae_f = torch.mean(torch.abs(_logmag(Xt) - _logmag(x0)), dim=(1, 2))\n","    var_t = torch.var(Xt, dim=(1, 2), unbiased=False)\n","    return mae_t.cpu().numpy(), mae_f.cpu().numpy(), var_t.cpu().numpy()\n","\n","# It writes window-level anomaly predictions back to 15-minute timestamps and saves them.\n","# Recall, we had separated into merged 96-points of 15-minutes each to a day window\n","def write_preds_15m(out_path, series_15m, starts, ends, pred_labels, original_df):\n","    original_df = original_df.copy()\n","    original_df[TIME_COL] = pd.to_datetime(original_df[TIME_COL], errors=\"coerce\")\n","    original_df = original_df.dropna(subset=[TIME_COL]).sort_values(TIME_COL)\n","    gt_cols = [\n","        c\n","        for c in original_df.columns\n","        if c.lower() in [\"ground_truth_anomaly\", \"ground_truth_appliance\"]\n","    ]\n","    original_gt = (\n","        original_df[[TIME_COL] + gt_cols].drop_duplicates(subset=[TIME_COL])\n","        if gt_cols\n","        else original_df[[TIME_COL]].copy()\n","    )\n","    frames = []\n","    for i in range(len(starts)):\n","        idx = pd.date_range(starts[i], ends[i], freq=\"15min\")\n","        seg = series_15m.reindex(idx).interpolate(\"time\").ffill().bfill()\n","        seg_df = pd.DataFrame({TIME_COL: seg.index, VAL_COL: seg.values})\n","        seg_df = seg_df.merge(original_gt, how=\"left\", on=TIME_COL)\n","        seg_df[\"prediction_anomaly\"] = pred_labels[i]\n","        frames.append(seg_df)\n","    out_df = (\n","        pd.concat(frames, ignore_index=True)\n","        if frames\n","        else pd.DataFrame(\n","            columns=[\n","                TIME_COL,\n","                VAL_COL,\n","                \"ground_truth_anomaly\",\n","                \"ground_truth_appliance\",\n","                \"prediction_anomaly\",\n","            ]\n","        )\n","    )\n","    for c in [\"ground_truth_anomaly\", \"ground_truth_appliance\"]:\n","        if c not in out_df.columns:\n","            out_df[c] = np.nan\n","    out_df = out_df[\n","        [TIME_COL, VAL_COL, \"ground_truth_anomaly\", \"ground_truth_appliance\", \"prediction_anomaly\"]\n","    ]\n","    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n","    out_df.to_csv(out_path, index=False)\n","    return out_df\n","\n","# Determine the following metrics: Accuracy/Precision/Recall/F1-Score/Normal_pct/Anomaly_pct/Total/TP/TN/FP/FN/ActualNormal/ActualAnomaly\n","def metrics_from_rows(df):\n","    cols = df.columns\n","    if (\"ground_truth_anomaly\" not in cols) or (\"prediction_anomaly\" not in cols):\n","        return dict(Accuracy=np.nan, Precision=np.nan, Recall=np.nan, F1_Score=np.nan,\n","                    Normal_pct=np.nan, Anomaly_pct=np.nan, Total=0,\n","                    TP=0, TN=0, FP=0, FN=0, ActualNormal=0, ActualAnomaly=0)\n","    def gt_flag(x):\n","        if pd.isna(x): return np.nan\n","        s = str(x).lower()\n","        if \"anom\" in s or s in [\"1\", \"true\", \"t\", \"yes\", \"y\"]: return 1\n","        if s in [\"0\", \"false\", \"f\", \"no\", \"n\", \"normal\", \"norm\"]: return 0\n","        return np.nan\n","    df = df.copy()\n","    df[\"gt\"] = df[\"ground_truth_anomaly\"].map(gt_flag).astype(\"float32\")\n","    df[\"pd\"] = df[\"prediction_anomaly\"].map(lambda x: 1 if str(x).lower() == \"anomaly\" else 0).astype(\"float32\")\n","    df = df.dropna(subset=[\"gt\"])\n","    if len(df) == 0:\n","        return dict(Accuracy=np.nan, Precision=np.nan, Recall=np.nan, F1_Score=np.nan,\n","                    Normal_pct=np.nan, Anomaly_pct=np.nan, Total=0,\n","                    TP=0, TN=0, FP=0, FN=0, ActualNormal=0, ActualAnomaly=0)\n","    gt, pdv = df[\"gt\"].values.astype(int), df[\"pd\"].values.astype(int)\n","    TP = int(((gt == 1) & (pdv == 1)).sum())\n","    TN = int(((gt == 0) & (pdv == 0)).sum())\n","    FP = int(((gt == 0) & (pdv == 1)).sum())\n","    FN = int(((gt == 1) & (pdv == 0)).sum())\n","    tot = len(gt)\n","    actN, actA = int((gt == 0).sum()), int((gt == 1).sum())\n","    acc = (TP + TN) / tot if tot > 0 else np.nan\n","    prec = TP / (TP + FP) if (TP + FP) > 0 else np.nan\n","    rec = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n","    f1 = 2 * prec * rec / (prec + rec) if prec and rec else np.nan\n","    norm_pct = (TN / actN) * 100 if actN > 0 else np.nan\n","    anom_pct = (TP / actA) * 100 if actA > 0 else np.nan\n","    return dict(Accuracy=acc, Precision=prec, Recall=rec, F1_Score=f1,\n","                Normal_pct=norm_pct, Anomaly_pct=anom_pct, Total=tot,\n","                TP=TP, TN=TN, FP=FP, FN=FN,\n","                ActualNormal=actN, ActualAnomaly=actA)\n","\n","# Save into summary file\n","def append_summary(res, row):\n","    path = f\"{SUMMARY_DIR}/{res}_ANOMALY_DIFFUSION_OUTLINE.csv\"\n","    pd.DataFrame([row]).to_csv(path, mode=\"a\", header=not os.path.exists(path), index=False)\n","\n","# -----------------------------\n","# TRAINING PROCESS\n","# -----------------------------\n","def train_residence(res):\n","    # Get the data - 15-minutes, windows, 80%, and standardize\n","    train_csv = f\"{MERGED_DIR}/{res}_Fridge_15minutes_StepChange_MERGED.csv\"\n","    if not os.path.exists(train_csv): raise FileNotFoundError(train_csv)\n","    tracemalloc.start(); t0 = time.time()\n","    s_tr = load_series_15T(train_csv)\n","    X, _, _ = daily_windows(s_tr)\n","    n, ntr = len(X), int(0.8 * len(X))\n","    if not (0 < ntr < n): raise ValueError(f\"{res}: need >=2 full-day windows (got {n}).\")\n","    Xtr, Xte = X[:ntr], X[ntr:]\n","    Tr, Te, mu, sd = standardize(Xtr, Xte)\n","\n","    # Build the model\n","    net, sch, opt = build_model()\n","    Tr_t = torch.tensor(Tr, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n","\n","    # Training the Model\n","    for step in range(STEPS):\n","        if Tr_t.shape[0] == 0: break\n","        idx = torch.randint(0, Tr_t.shape[0], (min(BATCH, Tr_t.shape[0]),), device=DEVICE)\n","        x = Tr_t.index_select(0, idx)\n","        t = torch.randint(0, TSTEPS, (x.shape[0],), dtype=torch.int64, device=DEVICE)\n","        n = torch.randn_like(x)\n","        noisy = sch.add_noise(x, n, t)\n","        loss = torch.mean((net(noisy, t).sample - n) ** 2)\n","        opt.zero_grad(); loss.backward(); opt.step()\n","        if (step + 1) % 100 == 0:\n","            print(f\"[{res}] Step {step+1}/{STEPS}, Loss = {loss.item():.6f}\")\n","    with torch.no_grad():\n","        tr_t, tr_f, tr_v = recon_err(net, sch, Tr_t)\n","\n","    # Find the thresholds\n","    zt, zf = robust_z(tr_t), robust_z(tr_f)\n","    tr_score = 0.5 * (zt + zf)\n","    thr = float(np.percentile(tr_score, THR_PCT))\n","    var_thr = float(np.percentile(tr_v, 1.0))\n","    train_time = time.time() - t0\n","    _, peak = tracemalloc.get_traced_memory()\n","    tracemalloc.stop()\n","    train_peak_mb = peak / (1024 * 1024)\n","    with open(f\"{OUT_DIR}/training_metadata_{res}.json\", \"w\") as f:\n","        json.dump(dict(residence=res, threshold=thr, var_threshold=var_thr,\n","                       train_peak_mb=train_peak_mb, train_time=train_time), f, indent=2)\n","    return net, sch, mu, sd, thr, var_thr, train_time, train_peak_mb\n","\n","# -----------------------------\n","# INFERENCE PROCESS\n","# -----------------------------\n","def test_file(csv_path, res, app, anom, thr, var_thr, mu, sd, net, sch, t_train, p_train):\n","    # Get the data - 15-minutes, windows, and standardize\n","    tracemalloc.start(); t1 = time.time()\n","    s = load_series_15T(csv_path)\n","    X, starts, ends = daily_windows(s)\n","    Xn = ((X - mu) / sd).astype(\"float32\") if len(X) > 0 else X\n","    Xt = torch.tensor(Xn, dtype=torch.float32, device=DEVICE).unsqueeze(1) if len(X) > 0 else torch.zeros((0, 1, WIN), device=DEVICE)\n","\n","    # Determine Reconstruction Error\n","    with torch.no_grad():\n","        te_t, te_f, te_v = recon_err(net, sch, Xt)\n","\n","    # Calculate the score and determine if it is an anomaly\n","    score = 0.5 * (robust_z(te_t) + robust_z(te_f))\n","    is_anom = (score > thr) | (te_v < var_thr)\n","    preds = np.where(is_anom, \"Anomaly\", \"Normal\")\n","\n","    # Print to files\n","    out_path = f\"{OUT_DIR}/{res}_{app}_15minutes_{anom}_MERGED_DIFFUSION_RESIDUALSPECTRAL.csv\"\n","    per15 = write_preds_15m(out_path, s, starts, ends, preds, pd.read_csv(csv_path))\n","    m = metrics_from_rows(per15)\n","    inf_time = time.time() - t1\n","    _, peak = tracemalloc.get_traced_memory()\n","    tracemalloc.stop()\n","    inf_peak_mb = peak / (1024 * 1024)\n","    row = dict(Residence=res, Appliance=app, AnomalyType=anom,\n","               ThresholdPct=THR_PCT, ThresholdValue=thr, VarThreshold=var_thr,\n","               Windows=len(preds), TrainingTimeSec=round(t_train, 6),\n","               InferenceTimeSec=round(inf_time, 6), TrainPeakMB=round(p_train, 3),\n","               InferencePeakMB=round(inf_peak_mb, 3), **m)\n","    return row\n","\n","# Do testing per file (appliance/anomaly)\n","def test_residence(res, net, sch, mu, sd, thr, var_thr, t_train, p_train):\n","    for app in APPLIANCES:\n","        for anom in ANOMALIES:\n","            csv = f\"{MERGED_DIR}/{res}_{app}_15minutes_{anom}_MERGED.csv\"\n","            if not os.path.exists(csv):\n","                continue\n","            try:\n","                row = test_file(csv, res, app, anom, thr, var_thr, mu, sd, net, sch, t_train, p_train)\n","            except Exception as e:\n","                row = dict(Residence=res, Appliance=app, AnomalyType=anom, Error=str(e))\n","            append_summary(res, row)\n","\n","\n","# ---------------- MAIN ----------------\n","if __name__ == \"__main__\":\n","    for res in RESIDENCES:\n","        print(f\"\\n===== {res}: train & test =====\")\n","        try:\n","            net, sch, mu, sd, thr, var_thr, t_train, p_train = train_residence(res)\n","            test_residence(res, net, sch, mu, sd, thr, var_thr, t_train, p_train)\n","        except Exception as e:\n","            append_summary(res, dict(Residence=res, Error=f\"TRAIN ERROR: {e}\"))\n","        finally:\n","            try:\n","                del net, sch\n","            except:\n","                pass\n","            gc.collect()\n","            if torch.cuda.is_available():\n","                torch.cuda.empty_cache()\n","\n","    print(\"\\nDone. 15-minute predictions saved, summaries in Percentiles_Summary/\")\n"]}]}