{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1-ue_NcryhZiQ78XAVUvkmmQUSqCR6cgR","authorship_tag":"ABX9TyOyo9wvuSp9mVmCC9zbhyJB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["##### ANOMALY DETECTION WITH ISOLATION FOREST #####\n","#\n","# \"Anomaly detection using random trees to isolate outliers.\"\n","# \"It isolates anomalies faster (lower tree levels) because they require fewer random splits than normal points.\"\n","#\n","#\n","### Training algorithm (per residence)\n","# 1. Locate training file: {residence}_Fridge_15minutes_StepChange_MERGED.csv\n","# 2. Load + normalize\n","#    - Read CSV into a dataframe.\n","#    - Ensure timestamp exists, parse to datetime, sort by time.\n","#    - Ensure active_power is numeric, replace inf with NaN.\n","#    - Fill small gaps using backward/forward fill, then drop any remaining NaNs.\n","# 3. Time-ordered split (80/20)\n","#    - Take the first 80% of rows as the training portion.\n","#    - Take the last 20% of rows as the held-out test portion.\n","# 4. Fit Isolation Forest on training active power\n","#    - Use only the active_power column.\n","#    - Train IsolationForest(n_estimators=100, contamination=\"auto\", random_state=42, n_jobs=-1).\n","# 5. Track training efficiency - time / memory\n","#\n","#\n","### Inference algorithm (per residence, per file)\n","# 1. Collect all files to score\n","#    - Find every file matching MERGED/{residence}*.csv.\n","# 2. For each file\n","#    1. Load + normalize\n","#       - Read CSV.\n","#       - Parse/sort timestamp.\n","#       - Clean active_power (numeric, remove inf, fill gaps, drop leftover NaNs).\n","#    2. Predict anomaly labels\n","#       - Run model.predict() on active_power for every row.\n","#       - Convert predictions to:\n","#         - \"Anomaly\" when model returns -1\n","#         - \"Normal\" when model returns 1\n","#       - Store in a new column: prediction_anomaly.\n","# 3. Track inference efficiency - time/memory\n","# 4. Save scored CSV\n","#    - Write to ANOMALY_ISOLATIONFOREST/ with suffix _ISOLATIONFOREST.csv.\n","# 5. Compute per-file metrics\n","#    - Convert ground truth to binary anomaly vs normal.\n","#    - Compare against prediction_anomaly.\n","#    - Compute accuracy/precision/recall/F1, confusion matrix, and per-class “hit rates” (Normal% and Anomaly_%).\n","# 3. Save per-residence summary\n","#"],"metadata":{"id":"uIYR_3_ZrhxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kW_yNL0p4dWg"},"outputs":[],"source":["# -----------------------------\n","# Configuration - Paths & Parameters\n","# -----------------------------\n","import os\n","import glob\n","import time\n","import math\n","import json\n","import warnings\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import IsolationForest\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","import tracemalloc\n","try:\n","    import resource  # Unix\n","    HAVE_RESOURCE = True\n","except Exception:\n","    HAVE_RESOURCE = False\n","try:\n","    import psutil    # Fallback\n","    HAVE_PSUTIL = True\n","except Exception:\n","    HAVE_PSUTIL = False\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","BASE = \"/content/drive/MyDrive/Paper02_14Datasets\"\n","MERGED_DIR = f\"{BASE}/MERGED\"\n","OUT_DIR = f\"{BASE}/ANOMALY_ISOLATIONFOREST\"\n","SUMMARY_DIR = f\"{OUT_DIR}/Percentiles_Summary\"\n","Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n","Path(SUMMARY_DIR).mkdir(parents=True, exist_ok=True)\n","\n","RESIDENCES = [\n","              \"REFIT_House01\",\n","              \"REFIT_House02\",\n","              \"REFIT_House03\",\n","              \"REFIT_House05\",\n","              \"REFIT_House07\",\n","              \"REFIT_House09\",\n","              \"REFIT_House15\",\n","              \"UKDALE_House01\",\n","              \"UKDALE_House02\",\n","              \"UKDALE_House05\",\n","              \"AMPds2_House01\",\n","              \"GREEND_House00\",\n","              \"GREEND_House01\",\n","              \"GREEND_House03\"\n","          ] #, \"REFITT_House03\", \"UKDALE_House01\", \"UKDALE_House05\"]\n","TRAIN_FILE_TEMPLATE = \"{residence}_Fridge_15minutes_StepChange_MERGED.csv\"  # training-only file per residence\n","RANDOM_STATE = 42\n","\n","# -----------------------------\n","# Helpers\n","# -----------------------------\n","# Make directories to store content\n","def safe_make_dirs_for(file_path: str):\n","    Path(os.path.dirname(file_path)).mkdir(parents=True, exist_ok=True)\n","\n","# Normalize: existence/cleaning/timestamp & sort\n","def normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n","    # Ensure required columns exist and are clean\n","    expected_cols = [\"timestamp\", \"active_power\"]\n","    for col in expected_cols:\n","        if col not in df.columns:\n","            raise ValueError(f\"Missing required column '{col}' in input.\")\n","    # Convert timestamp & sort\n","    df = df.copy()\n","    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=False)\n","    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n","\n","    # Clean active_power\n","    df[\"active_power\"] = pd.to_numeric(df[\"active_power\"], errors=\"coerce\")\n","    df[\"active_power\"] = df[\"active_power\"].replace([np.inf, -np.inf], np.nan)\n","    # forward/backward fill small gaps, then finally drop leftover NaNs\n","    df[\"active_power\"] = df[\"active_power\"].bfill().ffill()\n","    df = df.dropna(subset=[\"active_power\"])\n","    return df\n","\n","# Return ground_truth_anomaly as binary (1=Anomaly, 0=Normal)\n","def y_from_ground_truth(df: pd.DataFrame):\n","    if \"ground_truth_anomaly\" not in df.columns:\n","        return None, False\n","    col = df[\"ground_truth_anomaly\"]\n","\n","    def map_val(v):\n","        if isinstance(v, str):\n","            v = v.strip().lower()\n","            if v == \"anomaly\": return 1\n","            # everything else counts as Normal\n","            return 0\n","        try:\n","            # treat any nonzero as anomaly\n","            return 1 if float(v) != 0 else 0\n","        except Exception:\n","            return 0\n","    y = col.map(map_val).astype(int)\n","    return y.values, True\n","\n","# Converts to binary labels (0 or 1) and readable labels (\"Normal\" or \"Anomaly\")\n","def preds_to_labels(preds_1normal_minus1anom):\n","    bin_ = np.where(preds_1normal_minus1anom == -1, 1, 0)\n","    str_ = np.where(bin_==1, \"Anomaly\", \"Normal\")\n","    return bin_, str_\n","\n","# Compute metrics: Accuracy/Precision/Recall/F1-Score/TN/FP/FN/TP/ActualNormal/ActualAnomaly/Normal%/Anomaly_%/Total(total anomaly)\n","def compute_metrics(y_true_bin, y_pred_bin):\n","    # y_* are {0,1}; handle cases with a single class in y_true safely\n","    labels_present = list(sorted(set(y_true_bin)))\n","    average = \"binary\" if set(labels_present) == {0,1} else \"micro\"\n","    acc = accuracy_score(y_true_bin, y_pred_bin)\n","    try:\n","        prec = precision_score(y_true_bin, y_pred_bin, zero_division=0)\n","        rec  = recall_score(y_true_bin, y_pred_bin, zero_division=0)\n","        f1   = f1_score(y_true_bin, y_pred_bin, zero_division=0)\n","    except Exception:\n","        # Fallback if class imbalance is extreme\n","        prec = rec = f1 = 0.0\n","\n","    # Confusion matrix with labels [0,1] (Normal, Anomaly)\n","    cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])\n","    # Handle shape safety (in case one class absent)\n","    if cm.shape == (2,2):\n","        tn, fp, fn, tp = cm.ravel()\n","    else:\n","        # Degenerate case\n","        tn = fp = fn = tp = 0\n","        if labels_present == [0]:\n","            tn = int((y_true_bin==0).sum())\n","        elif labels_present == [1]:\n","            tp = int((y_true_bin==1).sum())\n","\n","    actual_normal  = int((y_true_bin==0).sum())\n","    actual_anomaly = int((y_true_bin==1).sum())\n","    normal_pct  = (tn / actual_normal * 100.0) if actual_normal > 0 else 0.0\n","    anomaly_pct = (tp / actual_anomaly * 100.0) if actual_anomaly > 0 else 0.0\n","    total = len(y_true_bin)\n","\n","    return {\n","        \"Accuracy\": acc,\n","        \"Precision\": prec,\n","        \"Recall\": rec,\n","        \"F1-Score\": f1,\n","        \"TN\": int(tn),\n","        \"FP\": int(fp),\n","        \"FN\": int(fn),\n","        \"TP\": int(tp),\n","        \"ActualNormal\": int(actual_normal),\n","        \"ActualAnomaly\": int(actual_anomaly),\n","        \"Normal%\": normal_pct,\n","        \"Anomaly_%\": anomaly_pct,\n","        \"Total\": int(total),\n","    }\n","\n","# Memory functions\n","def start_mem_trace():\n","    tracemalloc.start()\n","def stop_mem_trace_mb():\n","    current, peak = tracemalloc.get_traced_memory()\n","    tracemalloc.stop()\n","    # Convert bytes to MB\n","    return peak / (1024*1024)\n","def alt_peak_mb():\n","    # Best-effort platform peak memory (not always available)\n","    if HAVE_RESOURCE:\n","        # ru_maxrss: on Linux returns KB, on macOS bytes. Normalize cautiously:\n","        ru = resource.getrusage(resource.RUSAGE_SELF)\n","        peak = ru.ru_maxrss\n","        # Heuristic: if very large, assume bytes (macOS), else KB (Linux)\n","        if peak > 10**9:  # bytes\n","            return peak / (1024*1024)\n","        else:             # KB\n","            return peak / 1024.0\n","    if HAVE_PSUTIL:\n","        proc = psutil.Process(os.getpid())\n","        mem = getattr(proc.memory_info(), \"rss\", 0)\n","        return mem / (1024*1024)\n","    return None\n","\n","# Training the Isolation forest\n","# contaminaton=\"auto\" -> all data is normal, RANDOM_STATE for repeatability\n","def fit_isolation_forest(train_active_power: np.ndarray):\n","    model = IsolationForest(\n","        n_estimators=100,\n","        contamination=\"auto\",\n","        random_state=RANDOM_STATE,\n","        n_jobs=-1,\n","    )\n","    # Train time + mem\n","    start = time.perf_counter()\n","    start_mem_trace()\n","    model.fit(train_active_power.reshape(-1, 1))\n","    train_peak_mb_trace = stop_mem_trace_mb()\n","    train_time_sec = time.perf_counter() - start\n","\n","    # Alternate peak (if available)\n","    train_peak_mb_alt = alt_peak_mb()\n","    train_peak_mb = train_peak_mb_trace if train_peak_mb_trace is not None else (train_peak_mb_alt or 0.0)\n","\n","    return model, train_time_sec, train_peak_mb\n","\n","# Make prediction from the IsolationForest and also record memory/time\n","def predict_with_timing(model, x_array: np.ndarray):\n","    start = time.perf_counter()\n","    start_mem_trace()\n","    preds = model.predict(x_array.reshape(-1,1))  # 1=inlier, -1=outlier\n","    infer_peak_mb_trace = stop_mem_trace_mb()\n","    infer_time_sec = time.perf_counter() - start\n","\n","    infer_peak_mb_alt = alt_peak_mb()\n","    infer_peak_mb = infer_peak_mb_trace if infer_peak_mb_trace is not None else (infer_peak_mb_alt or 0.0)\n","    return preds, infer_time_sec, infer_peak_mb\n","\n","# Make predictions (with predict_with_timing) and records the metrics\n","def evaluate_on_test(df_test: pd.DataFrame, model):\n","    y_true_bin, has_gt = y_from_ground_truth(df_test)\n","    metrics = {}\n","    if has_gt:\n","        preds_raw, infer_time, infer_peak = predict_with_timing(model, df_test[\"active_power\"].values.astype(float))\n","        y_pred_bin, _ = preds_to_labels(preds_raw)\n","        metrics = compute_metrics(y_true_bin, y_pred_bin)\n","        metrics.update({\n","            \"InferenceTimeSec\": float(infer_time),\n","            \"InferencePeakMB\": float(infer_peak),\n","        })\n","    else:\n","        metrics = {\n","            \"Accuracy\": np.nan, \"Precision\": np.nan, \"Recall\": np.nan, \"F1-Score\": np.nan,\n","            \"TN\": np.nan, \"FP\": np.nan, \"FN\": np.nan, \"TP\": np.nan,\n","            \"ActualNormal\": np.nan, \"ActualAnomaly\": np.nan,\n","            \"Normal%\": np.nan, \"Anomaly_%\": np.nan,\n","            \"Total\": len(df_test),\n","            \"InferenceTimeSec\": np.nan, \"InferencePeakMB\": np.nan,\n","        }\n","    return metrics\n","\n","# Make predictions and save\n","def add_predictions_and_save(model, in_path: str, residence: str):\n","    df = pd.read_csv(in_path)\n","    df = normalize_df(df)\n","\n","    # Predict (full file) with timing\n","    preds_raw, infer_time, infer_peak = predict_with_timing(model, df[\"active_power\"].values.astype(float))\n","    _, preds_text = preds_to_labels(preds_raw)\n","    df[\"prediction_anomaly\"] = preds_text\n","\n","    # Save to required location/name\n","    # Turn: MERGED/{residence}*.csv  -> ANOMALY_ISOLATIONFOREST/{residence}*_ISOLATIONFOREST.csv\n","    in_name = os.path.basename(in_path)\n","    out_name = f\"{os.path.splitext(in_name)[0]}_ISOLATIONFOREST.csv\"\n","    out_path = os.path.join(OUT_DIR, out_name)\n","    safe_make_dirs_for(out_path)\n","    df.to_csv(out_path, index=False)\n","\n","    # If ground truth exists, compute per-file metrics too\n","    y_true_bin, has_gt = y_from_ground_truth(df)\n","    if has_gt:\n","        y_pred_bin = (df[\"prediction_anomaly\"].str.lower() == \"anomaly\").astype(int).values\n","        metrics = compute_metrics(y_true_bin, y_pred_bin)\n","    else:\n","        metrics = {\n","            \"Accuracy\": np.nan, \"Precision\": np.nan, \"Recall\": np.nan, \"F1-Score\": np.nan,\n","            \"TN\": np.nan, \"FP\": np.nan, \"FN\": np.nan, \"TP\": np.nan,\n","            \"ActualNormal\": np.nan, \"ActualAnomaly\": np.nan,\n","            \"Normal%\": np.nan, \"Anomaly_%\": np.nan,\n","            \"Total\": len(df),\n","        }\n","\n","    # Attach inference timing/memory for this file\n","    metrics.update({\n","        \"InferenceTimeSec\": float(infer_time),\n","        \"InferencePeakMB\": float(infer_peak),\n","    })\n","\n","    return out_path, metrics\n","\n","# -----------------------------\n","# Main - Loop through residences\n","# -----------------------------\n","all_residence_reports = {}\n","\n","for residence in RESIDENCES:\n","    print(f\"\\n=== Residence: {residence} ===\")\n","    train_file = os.path.join(MERGED_DIR, TRAIN_FILE_TEMPLATE.format(residence=residence))\n","    if not os.path.exists(train_file):\n","        print(f\"!! Training file not found: {train_file}. Skipping this residence.\")\n","        continue\n","\n","    # Load and clean training file\n","    df_train_full = pd.read_csv(train_file)\n","    df_train_full = normalize_df(df_train_full)\n","\n","    # Split 80/20 by time order\n","    n = len(df_train_full)\n","    split_idx = int(0.8 * n)\n","    df_train = df_train_full.iloc[:split_idx].copy()\n","    df_test  = df_train_full.iloc[split_idx:].copy()\n","\n","    # Train ONLY on active_power (assumed all normal)\n","    model, train_time_sec, train_peak_mb = fit_isolation_forest(df_train[\"active_power\"].values.astype(float))\n","\n","    # Evaluate on the held-out 20% (if GT available)\n","    test_metrics = evaluate_on_test(df_test, model)\n","\n","    # Scan every {residence}*.csv in MERGED and create predictions\n","    pattern = os.path.join(MERGED_DIR, f\"{residence}*.csv\")\n","    files = sorted(glob.glob(pattern))\n","    if not files:\n","        print(f\"No files found for pattern: {pattern}\")\n","        continue\n","\n","    rows = []\n","    for f in files:\n","        out_path, file_metrics = add_predictions_and_save(model, f, residence)\n","        # Compose summary row\n","        row = {\n","            \"Filename\": os.path.basename(f),\n","            \"Accuracy\": file_metrics[\"Accuracy\"],\n","            \"Precision\": file_metrics[\"Precision\"],\n","            \"Recall\": file_metrics[\"Recall\"],\n","            \"F1-Score\": file_metrics[\"F1-Score\"],\n","            \"Normal%\": file_metrics[\"Normal%\"],\n","            \"Anomaly_%\": file_metrics[\"Anomaly_%\"],\n","            \"TrainingTimeSec\": float(train_time_sec),\n","            \"InferenceTimeSec\": float(file_metrics[\"InferenceTimeSec\"]),\n","            \"TrainPeakMB\": float(train_peak_mb),\n","            \"InferencePeakMB\": float(file_metrics[\"InferencePeakMB\"]),\n","            \"Total\": file_metrics[\"Total\"],\n","            \"TP\": file_metrics[\"TP\"] if not (isinstance(file_metrics[\"TP\"], float) and math.isnan(file_metrics[\"TP\"])) else 0,\n","            \"TN\": file_metrics[\"TN\"] if not (isinstance(file_metrics[\"TN\"], float) and math.isnan(file_metrics[\"TN\"])) else 0,\n","            \"FP\": file_metrics[\"FP\"] if not (isinstance(file_metrics[\"FP\"], float) and math.isnan(file_metrics[\"FP\"])) else 0,\n","            \"FN\": file_metrics[\"FN\"] if not (isinstance(file_metrics[\"FN\"], float) and math.isnan(file_metrics[\"FN\"])) else 0,\n","            \"ActualNormal\": file_metrics[\"ActualNormal\"] if not (isinstance(file_metrics[\"ActualNormal\"], float) and math.isnan(file_metrics[\"ActualNormal\"])) else 0,\n","            \"ActualAnomaly\": file_metrics[\"ActualAnomaly\"] if not (isinstance(file_metrics[\"ActualAnomaly\"], float) and math.isnan(file_metrics[\"ActualAnomaly\"])) else 0,\n","        }\n","        rows.append(row)\n","        print(f\"Wrote predictions: {out_path}\")\n","\n","    # Save outline CSV for this residence\n","    outline_df = pd.DataFrame(rows)\n","    outline_path = os.path.join(SUMMARY_DIR, f\"{residence}_ANOMALY_ISOLATIONFOREST_OUTLINE.csv\")\n","    safe_make_dirs_for(outline_path)\n","    outline_df.to_csv(outline_path, index=False)\n","    all_residence_reports[residence] = outline_path\n","    print(f\"Summary saved: {outline_path}\")\n","\n","# Final hint line so you can quickly find outputs\n","print(\"\\nDone. Per-residence outline CSVs:\")\n","for k, v in all_residence_reports.items():\n","    print(f\" - {k}: {v}\")\n","print(f\"\\nPredicted CSVs live under: {OUT_DIR}\")\n"]}]}