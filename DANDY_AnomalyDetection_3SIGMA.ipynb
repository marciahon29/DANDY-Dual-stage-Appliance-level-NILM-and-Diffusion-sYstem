{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"19DO7bEqAf-Mde-P5VyhbcRj2U13R1r4j","authorship_tag":"ABX9TyMThDnJ+n14TVHNzAiZxYLB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["##### ANOMALY DETECTION WITH 3-SIGMA #####\n","#\n","# This code performs Anomaly Detection with 3-SIGMA algorithm.\n","# Essentially, this translates to anomalies being greater/less than \"Mean +/- 3xStandardDeviation\"\n","#\n","### Training Process (per residence)\n","# 1. Select the training file\n","#    - Use: {residence}_Fridge_15minutes_StepChange_MERGED.csv\n","# 2. Load and clean the training data\n","#    - Read the CSV\n","#    - Convert active_power to numeric (invalid values become missing)\n","#    - Parse timestamp if present and sort rows chronologically (if parsing works)\n","#    - Normalize ground_truth_anomaly\n","# 3. reate the train split\n","#    - Take the first 80% of rows as the training portion (df_train)\n","# 4. Compute the 3-sigma thresholds from training active power\n","#    - Remove missing and infinite active_power values\n","#    - Compute the training mean and standard deviation\n","#    - If the standard deviation is zero, replace it with a tiny value to avoid degenerate thresholds\n","#    - Compute lower and upper using k = 3.0\n","# 5. Store training artifacts (in memory)\n","#    - Keep mu, lower, upper for this residence\n","#    - Record training time and peak memory (for reporting)\n","#\n","#\n","### Inference Algorithm (per residence - using the whole dataset)\n","# 1. Collect all files for the residence\n","#    - Match: {residence}*.csv in the MERGED directory\n","# 2. For each file\n","#    1. Load and clean the file\n","#       - Read the CSV\n","#       - Convert active_power to numeric\n","#       - Normalize timestamp formatting if present\n","#       - Normalize ground_truth_anomaly if present\n","#    2. Predict anomalies using the trained thresholds\n","#       - Label each row as:\n","#         - \"Anomaly\" if active_power is below lower or above upper threshold\n","#         - \"Normal\" otherwise\n","#       - Save the predictions as a new column: prediction_anomaly\n","# 3. Save the per-file prediction output\n","#    - Write the full CSV (original columns + prediction_anomaly) to:\n","#      - {original_filename}_3SIGMA.csv in the output directory\n","# 4. Compute metrics (only if ground truth exists)\n","#      - TP/TN/FP/FN, Accuracy, Precision, Recall, F1-Score, Normal_%, Anomaly_%\n","# 3. Finalize residence summary\n","#      - Record time and memory\n","#      - Save the residence summary (metrics/time/memory) CSV to the summary directory"],"metadata":{"id":"cXjgRSu8lEfK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hTFebZB_nKq5"},"outputs":[],"source":["# -----------------------------\n","# Config - Path / Parameters\n","# -----------------------------\n","import os\n","import glob\n","import time\n","import math\n","import tracemalloc\n","import pandas as pd\n","import numpy as np\n","\n","RESIDENCES = [\n","    \"REFIT_House01\",\n","    \"REFIT_House02\",\n","    \"REFIT_House03\",\n","    \"REFIT_House05\",\n","    \"REFIT_House07\",\n","    \"REFIT_House09\",\n","    \"REFIT_House15\",\n","    \"UKDALE_House01\",\n","    \"UKDALE_House02\",\n","    \"UKDALE_House05\",\n","    \"AMPds2_House01\",\n","    \"GREEND_House00\",\n","    \"GREEND_House01\",\n","    \"GREEND_House03\"\n","]\n","\n","INPUT_TRAIN_PATTERN = \"/content/drive/MyDrive/Paper02_14Datasets/MERGED/{residence}_Fridge_15minutes_StepChange_MERGED.csv\"\n","INPUT_ALL_PATTERN   = \"/content/drive/MyDrive/Paper02_14Datasets/MERGED/{residence}*.csv\"\n","\n","OUTPUT_DIR_PRED     = \"/content/drive/MyDrive/Paper02_14Datasets/ANOMALY_3SIGMA\"\n","OUTPUT_DIR_SUMMARY  = os.path.join(OUTPUT_DIR_PRED, \"Percentiles_Summary\")\n","\n","MODEL_NAME          = \"3SIGMA\"\n","SIGMA_K             = 3.0  # classic 3-sigma\n","\n","os.makedirs(OUTPUT_DIR_PRED, exist_ok=True)\n","os.makedirs(OUTPUT_DIR_SUMMARY, exist_ok=True)\n","\n","# -----------------------------\n","# Helpers\n","# -----------------------------\n","# Ensure that the colums have the correct formatting.\n","def read_csv_safe(path: str) -> pd.DataFrame:\n","    df = pd.read_csv(path)\n","    # Normalize columns defensively\n","    if \"timestamp\" in df.columns:\n","        try:\n","            ts = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n","            df[\"timestamp\"] = ts.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n","        except Exception:\n","            pass\n","    if \"active_power\" in df.columns:\n","        df[\"active_power\"] = pd.to_numeric(df[\"active_power\"], errors=\"coerce\")\n","    if \"ground_truth_anomaly\" in df.columns:\n","        df[\"ground_truth_anomaly\"] = df[\"ground_truth_anomaly\"].astype(str).str.strip()\n","        df.loc[~df[\"ground_truth_anomaly\"].eq(\"Anomaly\"), \"ground_truth_anomaly\"] = \"Normal\"\n","    return df\n","\n","# Determine the mu (mean), lower/upper thresholds (mu +/- 3xStandardDeviation)\n","def compute_train_thresholds(train_series: pd.Series, k: float = 3.0):\n","    x = pd.to_numeric(train_series, errors=\"coerce\").replace([np.inf, -np.inf], np.nan).dropna()\n","    if x.empty:\n","        return 0.0, -np.inf, np.inf\n","    mu = float(x.mean())\n","    sigma = float(x.std(ddof=0))\n","    if sigma == 0 or math.isclose(sigma, 0.0):\n","        sigma = 1e-9\n","    lower = mu - k * sigma\n","    upper = mu + k * sigma\n","    return mu, lower, upper\n","\n","# Predict Anomaly with threshold - Anomaly if above/below threshold\n","def predict_with_threshold(df: pd.DataFrame, lower: float, upper: float) -> pd.Series:\n","    ap = pd.to_numeric(df[\"active_power\"], errors=\"coerce\")\n","    pred = np.where((ap < lower) | (ap > upper), \"Anomaly\", \"Normal\")\n","    return pd.Series(pred, index=df.index, name=\"prediction_anomaly\")\n","\n","# Calculate all the Metrics\n","def safe_metrics(y_true: pd.Series, y_pred: pd.Series):\n","    yt = y_true.fillna(\"Normal\").astype(str)\n","    yp = y_pred.fillna(\"Normal\").astype(str)\n","    actual_anom_mask  = yt.eq(\"Anomaly\")\n","    actual_norm_mask  = yt.eq(\"Normal\")\n","    pred_anom_mask    = yp.eq(\"Anomaly\")\n","    pred_norm_mask    = yp.eq(\"Normal\")\n","\n","    TP = int(((actual_anom_mask) & (pred_anom_mask)).sum())\n","    TN = int(((actual_norm_mask) & (pred_norm_mask)).sum())\n","    FP = int(((actual_norm_mask) & (pred_anom_mask)).sum())\n","    FN = int(((actual_anom_mask) & (pred_norm_mask)).sum())\n","\n","    total = int(len(yt))\n","    actual_anom = int(actual_anom_mask.sum())\n","    actual_norm = int(actual_norm_mask.sum())\n","\n","    accuracy  = (TP + TN) / total if total else 0.0\n","    precision = TP / (TP + FP) if (TP + FP) else 0.0\n","    recall    = TP / (TP + FN) if (TP + FN) else 0.0\n","    f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n","\n","    normal_pct  = (TN / actual_norm * 100.0) if actual_norm else 0.0\n","    anomaly_pct = (TP / actual_anom * 100.0) if actual_anom else 0.0\n","\n","    return {\n","        \"Total\": total,\n","        \"TP\": TP, \"TN\": TN, \"FP\": FP, \"FN\": FN,\n","        \"ActualNormal\": actual_norm, \"ActualAnomaly\": actual_anom,\n","        \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1,\n","        \"Normal_%\": normal_pct, \"Anomaly_%\": anomaly_pct\n","    }\n","\n","# Calculate megabytes\n","def mb(bytes_val: int) -> float:\n","    return round(bytes_val / (1024 * 1024), 3)\n","\n","# Save and overwrite CSV\n","def save_csv_overwrite(df: pd.DataFrame, path: str):\n","    os.makedirs(os.path.dirname(path), exist_ok=True)\n","    df.to_csv(path, index=False)\n","\n","# -----------------------------\n","# Loop through residences\n","# -----------------------------\n","for residence in RESIDENCES:\n","    print(f\"\\n==== Processing {residence} ====\")\n","\n","    # -----------------------------\n","    # TRAINING Process\n","    # -----------------------------\n","    # Read the dataset\n","    train_path = INPUT_TRAIN_PATTERN.format(residence=residence)\n","    if not os.path.exists(train_path):\n","        print(f\"[WARN] Training file not found: {train_path}. Skipping residence.\")\n","        continue\n","\n","    df_train_full = read_csv_safe(train_path)\n","    if \"timestamp\" in df_train_full.columns:\n","        try:\n","            df_train_full[\"_ts_sort\"] = pd.to_datetime(df_train_full[\"timestamp\"], errors=\"coerce\")\n","            df_train_full = df_train_full.sort_values(\"_ts_sort\").drop(columns=[\"_ts_sort\"])\n","        except Exception:\n","            pass\n","\n","    # Obtain the firsrt 80% for training\n","    n = len(df_train_full)\n","    split_idx = int(n * 0.8)\n","    df_train = df_train_full.iloc[:split_idx].copy()\n","\n","    # Calculate mu/lower/upper thresholds along with memory and time\n","    tracemalloc.start()\n","    t0 = time.perf_counter()\n","    mu, lower, upper = compute_train_thresholds(df_train[\"active_power\"], k=SIGMA_K)\n","    train_time_sec = time.perf_counter() - t0\n","    train_current, train_peak = tracemalloc.get_traced_memory()\n","    tracemalloc.stop()\n","    train_peak_mb = mb(train_peak)\n","\n","    print(f\" Trained {MODEL_NAME}: mu={mu:.6f}, lower={lower:.6f}, upper={upper:.6f}\")\n","    print(f\" TrainingTimeSec={train_time_sec:.3f}, TrainPeakMB={train_peak_mb}\")\n","\n","    # -----------------------------\n","    # INFERENCE Process\n","    # -----------------------------\n","    pattern = INPUT_ALL_PATTERN.format(residence=residence)\n","    all_files = sorted(glob.glob(pattern))\n","    if not all_files:\n","        print(f\"[WARN] No files found for pattern: {pattern}\")\n","        continue\n","\n","    summary_rows = []\n","    tracemalloc.start()\n","    t_inf0 = time.perf_counter()\n","\n","    # Loop through all files\n","    for in_path in all_files:\n","        try:\n","            base = os.path.basename(in_path)\n","            out_path = os.path.join(OUTPUT_DIR_PRED, f\"{os.path.splitext(base)[0]}_{MODEL_NAME}.csv\")\n","\n","            df = read_csv_safe(in_path)\n","\n","            # Predict anomaly if above/below threshold\n","            df[\"prediction_anomaly\"] = predict_with_threshold(df, lower, upper)\n","            save_csv_overwrite(df, out_path)\n","\n","            # Calculate the metrics\n","            if \"ground_truth_anomaly\" in df.columns:\n","                m = safe_metrics(df[\"ground_truth_anomaly\"], df[\"prediction_anomaly\"])\n","            else:\n","                m = {\n","                    \"Total\": len(df), \"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0,\n","                    \"ActualNormal\": 0, \"ActualAnomaly\": 0,\n","                    \"Accuracy\": np.nan, \"Precision\": np.nan, \"Recall\": np.nan, \"F1-Score\": np.nan,\n","                    \"Normal_%\": np.nan, \"Anomaly_%\": np.nan\n","                }\n","\n","            summary_rows.append({\n","                \"Filename\": base,\n","                \"Accuracy\": m[\"Accuracy\"],\n","                \"Precision\": m[\"Precision\"],\n","                \"Recall\": m[\"Recall\"],\n","                \"F1-Score\": m[\"F1-Score\"],\n","                \"Normal_%\": m[\"Normal_%\"],\n","                \"Anomaly_%\": m[\"Anomaly_%\"],\n","                \"Total\": m[\"Total\"],\n","                \"TP\": m[\"TP\"], \"TN\": m[\"TN\"], \"FP\": m[\"FP\"], \"FN\": m[\"FN\"],\n","                \"ActualNormal\": m[\"ActualNormal\"], \"ActualAnomaly\": m[\"ActualAnomaly\"],\n","                \"TrainingTimeSec\": None,\n","                \"InferenceTimeSec\": None,\n","                \"TrainPeakMB\": train_peak_mb,\n","                \"InferencePeakMB\": None,\n","            })\n","\n","            print(f\"  ✔ Predicted & saved: {base}  |  Acc={m['Accuracy']:.4f}  N%={m['Normal_%']:.2f}  A%={m['Anomaly_%']:.2f}\")\n","\n","        except Exception as e:\n","            print(f\"  [ERROR] {in_path}: {e}\")\n","\n","    inference_time_sec = time.perf_counter() - t_inf0\n","    inf_current, inf_peak = tracemalloc.get_traced_memory()\n","    tracemalloc.stop()\n","    inference_peak_mb = mb(inf_peak)\n","\n","    for row in summary_rows:\n","        row[\"TrainingTimeSec\"] = round(train_time_sec, 6)\n","        row[\"InferenceTimeSec\"] = round(inference_time_sec, 6)\n","        row[\"InferencePeakMB\"] = inference_peak_mb\n","\n","    summary_df = pd.DataFrame(summary_rows)\n","    col_order = [\n","        \"Filename\",\n","        \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\",\n","        \"Normal_%\", \"Anomaly_%\", \"Total\",\n","        \"TP\", \"TN\", \"FP\", \"FN\", \"ActualNormal\", \"ActualAnomaly\",\n","        \"TrainingTimeSec\", \"InferenceTimeSec\", \"TrainPeakMB\", \"InferencePeakMB\"\n","    ]\n","    summary_df = summary_df[col_order]\n","\n","    summary_path = os.path.join(OUTPUT_DIR_SUMMARY, f\"{residence}_ANOMALY_{MODEL_NAME}_OUTLINE.csv\")\n","    save_csv_overwrite(summary_df, summary_path)\n","\n","    print(f\" InferenceTimeSec={inference_time_sec:.3f}, InferencePeakMB={inference_peak_mb}\")\n","    print(f\" ✔ Summary saved: {summary_path}\")\n","\n","print(\"\\nDone.\")\n"]}]}