{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1evlUapfOe4zPeGaMcGhiQaLz2w2l--iD","authorship_tag":"ABX9TyM+FcpFXXgEw2pazwKeDZRn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["##### ANOMALY DETECTION WITH COCA (Contrastive One-Class Anomaly detection method of time series) #####\n","#\n","# This code was adopted from: https://github.com/ruiking04/COCA\n","#\n","# The description of COCA is in paper: https://arxiv.org/abs/2207.01472\n","#"],"metadata":{"id":"kNnKNTbLJmGW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Maybe need to run this\n","!pip install sympy==1.12"],"metadata":{"id":"L5_XpVhHW0DD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWgaIG0QUSgJ"},"outputs":[],"source":["# -----------------------------#\n","# CONFIG - PATHS and PARAMETERS\n","# -----------------------------#\n","import os, glob, time, math, gc, tracemalloc, warnings\n","import numpy as np\n","import pandas as pd\n","from typing import Tuple, List\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","RESIDENCES = [\"REFITT_House02\"] #, \"REFITT_House03\", \"UKDALE_House01\", \"UKDALE_House05\"]\n","\n","BASE_MERGED_DIR = \"/content/drive/MyDrive/Paper02_REFITT_UKDALE/MERGED\"\n","BASE_OUT_DIR    = \"/content/drive/MyDrive/Paper02_REFITT_UKDALE/ANOMALY_COCA_v2\"\n","SUMMARY_DIR     = os.path.join(BASE_OUT_DIR, \"Percentiles_Summary\")\n","\n","os.makedirs(BASE_OUT_DIR, exist_ok=True)\n","os.makedirs(SUMMARY_DIR, exist_ok=True)\n","\n","# Sliding window + model hyperparams (kept simple)\n","WIN_LEN     = 64      # window length T (paper uses 64 on UCR; 16 on AIOps)\n","WIN_STRIDE  = 4       # step between windows\n","BATCH_SIZE  = 256\n","EPOCHS      = 10      # keep small for \"simplest possible\"\n","LR          = 3e-4\n","WEIGHT_DECAY= 5e-4\n","MU          = 0.1     # weight for variance term (<=1 recommended)\n","CE_FREEZE_EPOCHS = 5  # freeze center early to avoid collapse\n","AUGMENT     = True    # jitter + scaling (small) as per paper\n","\n","# Threshold quantile from *training* scores only (tune this to move Normal% / Anomaly%)\n","THRESH_Q    = 0.995   # 99.5th percentile is a common unsupervised choice\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.set_float32_matmul_precision(\"medium\")\n","\n","# -----------------------------#\n","# Utilities\n","# -----------------------------#\n","def standardize_fit(x: np.ndarray) -> Tuple[np.ndarray, float, float]:\n","    mu, sigma = float(np.nanmean(x)), float(np.nanstd(x) + 1e-8)\n","    return (x - mu) / sigma, mu, sigma\n","\n","def standardize_apply(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:\n","    return (x - mu) / (sigma + 1e-8)\n","\n","def make_windows(series: np.ndarray, win_len: int, stride: int) -> Tuple[np.ndarray, np.ndarray]:\n","    \"\"\"Return windows [N, win_len, 1] and index mapping (center of each window).\"\"\"\n","    xs, idx = [], []\n","    n = len(series)\n","    for start in range(0, n - win_len + 1, stride):\n","        end = start + win_len\n","        xs.append(series[start:end])\n","        idx.append(start + win_len // 2)  # center index\n","    X = np.asarray(xs, dtype=np.float32)[..., None]  # [N, win_len, 1]\n","    return X, np.asarray(idx, dtype=np.int64)\n","\n","def set_seed(seed: int = 42):\n","    import random\n","    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed);\n","    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n","set_seed(42)\n","\n","# -----------------------------#\n","# Model: Encoder (1D-Conv) → Seq2Seq (LSTM) → Projector (MLP)\n","# -----------------------------#\n","class TConvEncoder(nn.Module):\n","    def __init__(self, in_ch=1, c1=32, c2=64):\n","        super().__init__()\n","        self.block1 = nn.Sequential(\n","            nn.Conv1d(in_ch, c1, kernel_size=7, padding=3),\n","            nn.BatchNorm1d(c1), nn.ReLU(), nn.Dropout(0.15),\n","            nn.MaxPool1d(2)\n","        )\n","        self.block2 = nn.Sequential(\n","            nn.Conv1d(c1, c2, kernel_size=5, padding=2),\n","            nn.BatchNorm1d(c2), nn.ReLU(),\n","            nn.MaxPool1d(2)\n","        )\n","    def forward(self, x):  # x: [B, T, 1]\n","        x = x.transpose(1, 2)        # [B, 1, T]\n","        z = self.block2(self.block1(x))   # [B, C, L]\n","        z = z.transpose(1, 2)        # [B, L, C]\n","        return z\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, in_dim, hid=64, layers=3, dropout=0.45):\n","        super().__init__()\n","        self.encoder = nn.LSTM(in_dim, hid, num_layers=layers, batch_first=True, dropout=dropout)\n","        self.decoder = nn.LSTM(hid, hid, num_layers=layers, batch_first=True, dropout=dropout)\n","        self.to_z    = nn.Linear(hid, in_dim)  # project back to representation size\n","    def forward(self, z):  # z: [B, L, C]\n","        h, _ = self.encoder(z)        # [B, L, hid]\n","        out, _ = self.decoder(h)      # [B, L, hid]\n","        z_rec = self.to_z(out)        # [B, L, C]\n","        return z_rec\n","\n","class Projector(nn.Module):\n","    def __init__(self, dim, hidden=128, out_dim=128):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden), nn.BatchNorm1d(hidden), nn.ReLU(),\n","            nn.Linear(hidden, out_dim),\n","        )\n","    def forward(self, q):  # q: [B, L, C] -> collapse time by mean\n","        # Pool across time (mean) before projector; simple and stable\n","        q_mean = q.mean(dim=1)              # [B, C]\n","        return self.net(q_mean)             # [B, out_dim]\n","\n","class COCA(nn.Module):\n","    def __init__(self, in_ch=1, enc_c1=32, enc_c2=64, proj_dim=128):\n","        super().__init__()\n","        self.encoder = TConvEncoder(in_ch, enc_c1, enc_c2)\n","        rep_dim = enc_c2\n","        self.seq2seq = Seq2Seq(rep_dim)\n","        self.projector = Projector(rep_dim, hidden=128, out_dim=proj_dim)\n","        # running center in projection space\n","        self.register_buffer(\"center\", torch.zeros(proj_dim))\n","\n","    def forward(self, x):\n","        z  = self.encoder(x)            # [B, L, C]\n","        zt = self.seq2seq(z)            # [B, L, C]\n","        q  = self.projector(z)          # [B, D]\n","        qp = self.projector(zt)         # [B, D]\n","        return z, zt, q, qp\n","\n","# -----------------------------#\n","# COCA Loss: invariance + variance\n","# -----------------------------#\n","def _l2norm(x: torch.Tensor, eps: float=1e-9):\n","    return x / (x.norm(dim=1, keepdim=True) + eps)\n","\n","def coca_loss(q: torch.Tensor, qp: torch.Tensor, center: torch.Tensor,\n","              mu: float=MU) -> torch.Tensor:\n","    # All on unit sphere:\n","    qn  = _l2norm(q)\n","    qpn = _l2norm(qp)\n","    cn  = _l2norm(center.unsqueeze(0))\n","\n","    # Invariance term d(Q,Q') = mean([1 - cos(q, c)] + [1 - cos(q', c)])\n","    cos_q_c  = (qn * cn).sum(dim=1)\n","    cos_qp_c = (qpn * cn).sum(dim=1)\n","    d = (2.0 - cos_q_c - cos_qp_c).mean()\n","\n","    # Variance term v(Q) + v(Q')\n","    # Keep per-dimension std above gamma (~1.0). Use hinge on std.\n","    gamma, eps = 1.0, 1e-4\n","    std_q  = torch.sqrt(qn.var(dim=0) + eps)\n","    std_qp = torch.sqrt(qpn.var(dim=0) + eps)\n","    vq  = torch.clamp(gamma - std_q,  min=0).mean()\n","    vqp = torch.clamp(gamma - std_qp, min=0).mean()\n","    v = 0.5 * (vq + vqp)\n","\n","    return d + mu * v\n","\n","@torch.no_grad()\n","def anomaly_score(q: torch.Tensor, qp: torch.Tensor, center: torch.Tensor) -> np.ndarray:\n","    \"\"\"Si = 2 - cos(q, c) - cos(q', c)  (higher = more anomalous)\"\"\"\n","    qn  = _l2norm(q)\n","    qpn = _l2norm(qp)\n","    cn  = _l2norm(center.unsqueeze(0))\n","    cos_q_c  = (qn * cn).sum(dim=1)\n","    cos_qp_c = (qpn * cn).sum(dim=1)\n","    s = (2.0 - cos_q_c - cos_qp_c).cpu().numpy()\n","    return s\n","\n","# -----------------------------#\n","# Data loader\n","# -----------------------------#\n","class WindowDataset(torch.utils.data.Dataset):\n","    def __init__(self, X: np.ndarray, augment: bool=False):\n","        self.X = X.astype(np.float32)\n","        self.augment = augment\n","    def __len__(self): return len(self.X)\n","    def __getitem__(self, i):\n","        x = self.X[i]\n","        if self.augment:\n","            # jitter + scaling, small magnitudes (tune if desired)  :contentReference[oaicite:5]{index=5}\n","            if np.random.rand() < 0.9:\n","                x = x + np.random.normal(0, 0.01, size=x.shape).astype(np.float32)\n","            if np.random.rand() < 0.9:\n","                x = (1.0 + np.random.normal(0, 0.02)) * x\n","        return torch.from_numpy(x)\n","\n","# -----------------------------#\n","# Training & Inference\n","# -----------------------------#\n","def train_coca(train_windows: np.ndarray) -> Tuple[COCA, float, float, float]:\n","    model = COCA().to(DEVICE)\n","    opt   = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","\n","    ds = WindowDataset(train_windows, augment=AUGMENT)\n","    dl = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=0)\n","\n","    # init center from first batch projections\n","    with torch.no_grad():\n","        xb = next(iter(dl)).to(DEVICE)\n","        _, _, q, qp = model(xb)\n","        ce = 0.5 * (q.mean(dim=0) + qp.mean(dim=0))\n","        model.center.copy_(ce)\n","\n","    tracemalloc.start()\n","    t0 = time.perf_counter()\n","\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        for xb in dl:\n","            xb = xb.to(DEVICE)\n","            _, _, q, qp = model(xb)\n","\n","            # update center for first CE_FREEZE_EPOCHS using moving average\n","            with torch.no_grad():\n","                if epoch < CE_FREEZE_EPOCHS:\n","                    ce_batch = 0.5 * (q.mean(dim=0) + qp.mean(dim=0))\n","                    model.center.mul_(0.9).add_(0.1 * ce_batch)\n","\n","            loss = coca_loss(q, qp, model.center, mu=MU)\n","            opt.zero_grad(set_to_none=True)\n","            loss.backward()\n","            opt.step()\n","\n","        # (optional) tiny EMA weight decay or grad clip can be added for stability\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    train_time = time.perf_counter() - t0\n","    _, train_peak = tracemalloc.get_traced_memory()\n","    tracemalloc.stop()\n","    train_peak_mb = train_peak / (1024 * 1024)\n","\n","    return model, train_time, train_peak_mb, loss.item()\n","\n","@torch.no_grad()\n","def infer_scores(model: COCA, windows: np.ndarray) -> Tuple[np.ndarray, float, float]:\n","    ds = WindowDataset(windows, augment=False)\n","    dl = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","\n","    tracemalloc.start()\n","    t0 = time.perf_counter()\n","    scores = []\n","    model.eval()\n","    for xb in dl:\n","        xb = xb.to(DEVICE)\n","        _, _, q, qp = model(xb)\n","        s = anomaly_score(q, qp, model.center)\n","        scores.append(s)\n","    inf_time = time.perf_counter() - t0\n","    _, inf_peak = tracemalloc.get_traced_memory()\n","    tracemalloc.stop()\n","    inf_peak_mb = inf_peak / (1024 * 1024)\n","    return np.concatenate(scores, axis=0), inf_time, inf_peak_mb\n","\n","def pick_threshold_from_training(train_scores: np.ndarray, quantile: float=THRESH_Q) -> float:\n","    return float(np.quantile(train_scores, quantile))\n","\n","# -----------------------------#\n","# Metrics\n","# -----------------------------#\n","def evaluate_binary(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n","    # y_true: 0/1 where 1=\"Anomaly\"; y_pred: 0/1 predicted\n","    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n","    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n","    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n","    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n","    total = tp + tn + fp + fn\n","    acc = (tp + tn) / max(total, 1)\n","    prec = tp / max(tp + fp, 1)\n","    rec  = tp / max(tp + fn, 1)\n","    f1   = (2 * prec * rec) / max(prec + rec, 1e-12)\n","    actual_normal  = int((y_true == 0).sum())\n","    actual_anomaly = int((y_true == 1).sum())\n","    normal_pct  = tn / max(actual_normal, 1) if actual_normal > 0 else np.nan\n","    anomaly_pct = tp / max(actual_anomaly, 1) if actual_anomaly > 0 else np.nan\n","    return dict(\n","        Total=total, TP=tp, TN=tn, FP=fp, FN=fn,\n","        Accuracy=acc, Precision=prec, Recall=rec, F1=f1,\n","        ActualNormal=actual_normal, ActualAnomaly=actual_anomaly,\n","        NormalPct=normal_pct, AnomalyPct=anomaly_pct\n","    )\n","\n","# -----------------------------#\n","# End-to-end per residence\n","# -----------------------------#\n","def run_for_residence(residence: str):\n","    # 1) Load the main training CSV\n","    train_csv = os.path.join(BASE_MERGED_DIR, f\"{residence}_TV_15minutes_StepChange_MERGED.csv\")\n","    if not os.path.exists(train_csv):\n","        print(f\"[{residence}] Missing training file: {train_csv}\")\n","        return\n","\n","    df = pd.read_csv(train_csv)\n","    # Required cols\n","    for col in [\"timestamp\", \"active_power\", \"ground_truth_anomaly\"]:\n","        if col not in df.columns:\n","            raise ValueError(f\"[{residence}] Column `{col}` missing in {train_csv}\")\n","\n","    # Chronological split 80/20\n","    n = len(df)\n","    split = int(0.8 * n)\n","    df_train = df.iloc[:split].copy()\n","    df_test  = df.iloc[split:].copy()\n","\n","    # Standardize train, then apply to test\n","    x_train_std, mu, sigma = standardize_fit(df_train[\"active_power\"].astype(float).values)\n","    x_test_std  = standardize_apply(df_test[\"active_power\"].astype(float).values, mu, sigma)\n","\n","    # Windows\n","    Xtr, idx_tr = make_windows(x_train_std, WIN_LEN, WIN_STRIDE)\n","    Xte, idx_te = make_windows(x_test_std, WIN_LEN, WIN_STRIDE)\n","\n","    # 2) Train COCA\n","    model, train_time, train_peak_mb, last_loss = train_coca(Xtr)\n","\n","    # 3) Threshold from training only\n","    train_scores, _, _ = infer_scores(model, Xtr)\n","    tau = pick_threshold_from_training(train_scores, THRESH_Q)\n","\n","    # 4) Save per-file predictions & collect metrics into outline DataFrame\n","    outline_rows = []\n","\n","    # Include the training file itself as part of the “every file” loop so its metrics appear too\n","    pattern = os.path.join(BASE_MERGED_DIR, f\"{residence}*.csv\")\n","    files = sorted(glob.glob(pattern))\n","    if len(files) == 0:\n","        print(f\"[{residence}] No files matched: {pattern}\")\n","        return\n","\n","    for path in files:\n","        try:\n","            dfi = pd.read_csv(path)\n","            if \"active_power\" not in dfi.columns:\n","                print(f\"[{residence}] Skipping (no active_power): {os.path.basename(path)}\")\n","                continue\n","\n","            # standardize using *training* stats\n","            xi = standardize_apply(dfi[\"active_power\"].astype(float).values, mu, sigma)\n","\n","            # windows & scores\n","            Xi, idxi = make_windows(xi, WIN_LEN, WIN_STRIDE)\n","            scores, inf_time, inf_peak_mb = infer_scores(model, Xi)\n","\n","            # Map window scores back to row-level predictions by nearest-center assignment\n","            pred_series = np.zeros(len(dfi), dtype=np.float32)\n","            pred_flags  = np.zeros(len(dfi), dtype=np.int64)\n","            # Assign score at window-center positions; others filled by nearest previous center\n","            pred_series[idxi] = scores\n","            for i in range(len(dfi)):\n","                # nearest center index <= i\n","                pos = idxi[np.searchsorted(idxi, i, side=\"right\") - 1] if len(idxi) > 0 else 0\n","                pred_series[i] = pred_series[pos]\n","            y_pred = (pred_series > tau).astype(int)\n","\n","            # Write predictions file\n","            dfo = dfi.copy()\n","            dfo[\"prediction_anomaly\"] = np.where(y_pred == 1, \"Anomaly\", \"Normal\")\n","            out_name = os.path.splitext(os.path.basename(path))[0] + \"_COCA_v2.csv\"\n","            out_path = os.path.join(BASE_OUT_DIR, out_name)\n","            dfo.to_csv(out_path, index=False)\n","\n","            # Metrics if ground truth exists\n","            has_gt = \"ground_truth_anomaly\" in dfi.columns\n","            metrics = {}\n","            if has_gt:\n","                # normalize GT to 0/1\n","                gt = dfi[\"ground_truth_anomaly\"].astype(str).str.strip().str.lower()\n","                y_true = np.where(gt == \"anomaly\", 1, 0)\n","                metrics = evaluate_binary(y_true, y_pred)\n","            else:\n","                metrics = dict(\n","                    Total=len(dfi), TP=np.nan, TN=np.nan, FP=np.nan, FN=np.nan,\n","                    Accuracy=np.nan, Precision=np.nan, Recall=np.nan, F1=np.nan,\n","                    ActualNormal=np.nan, ActualAnomaly=np.nan,\n","                    NormalPct=np.nan, AnomalyPct=np.nan\n","                )\n","\n","            outline_rows.append({\n","                \"Filename\": os.path.basename(path),\n","                \"Accuracy\": metrics[\"Accuracy\"],\n","                \"Precision\": metrics[\"Precision\"],\n","                \"Recall\": metrics[\"Recall\"],\n","                \"F1-Score\": metrics[\"F1\"],\n","                \"Normal%\": metrics[\"NormalPct\"],\n","                \"Anomaly_%\": metrics[\"AnomalyPct\"],\n","                \"TrainingTimeSec\": train_time,\n","                \"InferenceTimeSec\": inf_time,\n","                \"TrainPeakMB\": train_peak_mb,\n","                \"InferencePeakMB\": inf_peak_mb,\n","                \"Total\": metrics[\"Total\"],\n","                \"TP\": metrics[\"TP\"], \"TN\": metrics[\"TN\"], \"FP\": metrics[\"FP\"], \"FN\": metrics[\"FN\"],\n","                \"ActualNormal\": metrics[\"ActualNormal\"], \"ActualAnomaly\": metrics[\"ActualAnomaly\"],\n","                \"Threshold\": tau,\n","                \"TrainLastLoss\": last_loss\n","            })\n","\n","            print(f\"[{residence}] Wrote: {out_path} | Acc={metrics['Accuracy']:.3f} F1={metrics['F1']:.3f} (if GT)\")\n","\n","        except Exception as e:\n","            print(f\"[{residence}] ERROR on {path}: {e}\")\n","\n","    # Save outline\n","    outline_df = pd.DataFrame(outline_rows)\n","    outline_path = os.path.join(SUMMARY_DIR, f\"{residence}_ANOMALY_COCA_v2_OUTLINE.csv\")\n","    outline_df.to_csv(outline_path, index=False)\n","    print(f\"[{residence}] Summary saved: {outline_path}\")\n","\n","# -----------------------------#\n","# Main\n","# -----------------------------#\n","if __name__ == \"__main__\":\n","    warnings.filterwarnings(\"ignore\")\n","    for res in RESIDENCES:\n","        run_for_residence(res)\n","    print(\"Done.\")\n"]}]}